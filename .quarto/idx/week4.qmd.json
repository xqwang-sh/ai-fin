{"title":"ç¬¬4è®²ï¼šå¤šæ¨¡æ€æ¨¡å‹ç†è®º â†’ é‡‘èä¿¡å·æŠ½å–","markdown":{"yaml":{"title":"ç¬¬4è®²ï¼šå¤šæ¨¡æ€æ¨¡å‹ç†è®º â†’ é‡‘èä¿¡å·æŠ½å–","subtitle":"å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘çš„ç‰¹å¾æå–ä¸å¢é‡ä¿¡æ¯æ£€éªŒ"},"headingText":"ä»»åŠ¡ä¸æ•°æ®ï¼šæŠŠå¤šæ¨¡æ€ç ”ç©¶è¡¨è¿°ä¸ºåˆ†ç±»é—®é¢˜","containsRefs":false,"markdown":"\n\n\n### ä¸ºä»€ä¹ˆæ˜¯\"åˆ†ç±»\"ï¼Ÿ\n\nè™½ç„¶å¤šæ¨¡æ€æ•°æ®å½¢å¼å¤šæ ·ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ï¼Œä½†åœ¨é‡‘èåº”ç”¨ä¸­ï¼Œæ ¸å¿ƒä»»åŠ¡é€šå¸¸å¯è¡¨è¿°ä¸º**åˆ†ç±»é—®é¢˜**ï¼š\n\n- é¢„æµ‹**æ¶¨/è·Œ**ï¼ˆäºŒåˆ†ç±»ï¼‰\n- è¯†åˆ«**ç®¡ç†å±‚æƒ…ç»ªçŠ¶æ€**ï¼ˆå¤šåˆ†ç±»ï¼šä¹è§‚/ä¸­æ€§/æ‚²è§‚ï¼‰\n- åˆ¤æ–­**æ¬ºè¯ˆ/éæ¬ºè¯ˆ**ï¼ˆäºŒåˆ†ç±»ï¼‰\n- é¢„æµ‹**ä¿¡ç”¨ç­‰çº§**ï¼ˆå¤šåˆ†ç±»ï¼‰\n\n:::{.callout-note icon=false}\nğŸ¯ æ ‡ç­¾å®šä¹‰çš„ä¸‰è¦ç´ \n\n1. **ä½•æ—¶**ï¼ˆWhenï¼‰ï¼šé¢„æµ‹å“ªä¸ªæ—¶æœŸçš„ç»“æœï¼Ÿ\n2. **å¯¹è°**ï¼ˆWhoï¼‰ï¼šä¸ªè‚¡/è¡Œä¸š/å¸‚åœºï¼Ÿ\n3. **é¢„æµ‹ä»€ä¹ˆ**ï¼ˆWhatï¼‰ï¼šå…·ä½“çš„ç±»åˆ«å®šä¹‰ï¼Ÿ\n\n**ç¤ºä¾‹**ï¼š\n\n- **ä½•æ—¶**ï¼šä¸šç»©ç”µè¯ä¼šåç¬¬äºŒä¸ªäº¤æ˜“æ—¥\n- **å¯¹è°**ï¼šè¯¥å…¬å¸è‚¡ç¥¨\n- **é¢„æµ‹ä»€ä¹ˆ**ï¼šæ”¶ç›Šç‡æ˜¯å¦è¶…è¿‡å¸‚åœºä¸­ä½æ•°ï¼ˆäºŒåˆ†ç±»ï¼‰\n\næ ‡ç­¾å®šä¹‰å†³å®šäº†ï¼š\n\n- æ•°æ®å¯¹é½æ–¹å¼ï¼ˆä½•æ—¶æå–ç‰¹å¾ï¼‰\n- è¯„ä¼°åè®®ï¼ˆæ—¶åº CV çš„çª—å£è®¾ç½®ï¼‰\n- æ ·æœ¬é€‰æ‹©ï¼ˆæ˜¯å¦åŒ…å«åœç‰Œè‚¡ç¥¨ï¼‰\n:::\n\n### å¤šæ¨¡æ€æ•°æ®ç‰ˆå›¾\n\n#### å›¾åƒï¼ˆImageï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **å«æ˜Ÿå›¾åƒ** | é¢„æµ‹é›¶å”®å®¢æµã€å·¥å‚æ´»åŠ¨ã€å•†å“åº“å­˜ | ç©ºé—´å¯¹é½ã€äº‘å±‚é®æŒ¡ |\n| **å¤œå…‰æ•°æ®** | åŒºåŸŸç»æµæ´»åŠ¨å¼ºåº¦ | åˆ†è¾¨ç‡ä½ã€å­£èŠ‚æ€§ |\n| **ç¥¨æ®/åˆåŒæ‰«æä»¶** | OCR æå–ä¿¡æ¯ã€æ¬ºè¯ˆæ£€æµ‹ | ç‰ˆå¼å¤šæ ·ã€å™ªå£°å¤§ |\n| **è´¢æŠ¥å›¾è¡¨æˆªå›¾** | è‡ªåŠ¨æå–æ•°æ®è¶‹åŠ¿ | ä¿¡æ¯æ³„éœ²é£é™©ï¼ˆéœ€ç¡®è®¤å‘å¸ƒæ—¶é—´ï¼‰|\n| **ç¤¾äº¤åª’ä½“å›¾ç‰‡** | å“ç‰Œå½¢è±¡ç›‘æµ‹ã€äº§å“è¯†åˆ« | æ ·æœ¬é€‰æ‹©åè¯¯ |\n\n#### éŸ³é¢‘ï¼ˆAudioï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **è´¢æŠ¥ç”µè¯ä¼š** | ç®¡ç†å±‚æƒ…ç»ªã€ä¸ç¡®å®šæ€§è¯†åˆ« | ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦» |\n| **å®¢æœå½•éŸ³** | å®¢æˆ·æ»¡æ„åº¦ã€æŠ•è¯‰é¢„è­¦ | éšç§åˆè§„ã€ä¿¡é“å™ªå£° |\n| **äº¤æ˜“å‘˜é€šè¯** | åˆè§„ç›‘æ§ã€å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ | å®æ—¶æ€§è¦æ±‚ã€æœ¯è¯­è¯†åˆ« |\n\n#### è§†é¢‘ï¼ˆVideoï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **ç®¡ç†å±‚è·¯æ¼”/å‘å¸ƒä¼š** | éè¨€è¯­çº¿ç´¢ï¼ˆé¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿ï¼‰| è§’åº¦/å…‰ç…§å˜åŒ–ã€é®æŒ¡ |\n| **ç›‘æ§è§†é¢‘** | ç½‘ç‚¹å®¢æµã€ATM å¼‚å¸¸ | è®¡ç®—æˆæœ¬é«˜ã€éšç§é—®é¢˜ |\n\n## è§†è§‰ï¼ˆå›¾åƒï¼‰æ¨¡æ€ï¼šç‰¹å¾æå–ä¸å˜é‡æ„é€ \n\n### å›¾åƒè¡¨ç¤ºæ–¹æ³•æ¼”è¿›\n\n#### ä¼ ç»Ÿæ–¹æ³•ï¼šæ‰‹å·¥ç‰¹å¾\n\n**ä½å±‚ç‰¹å¾**ï¼š\n\n- é¢œè‰²ç›´æ–¹å›¾\n- çº¹ç†ï¼ˆGabor æ»¤æ³¢å™¨ã€LBPï¼‰\n- è¾¹ç¼˜æ£€æµ‹ï¼ˆCannyã€Sobelï¼‰\n\n**ä¸­å±‚ç‰¹å¾**ï¼š\n\n- SIFTï¼ˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢ï¼‰\n- HOGï¼ˆæ–¹å‘æ¢¯åº¦ç›´æ–¹å›¾ï¼‰\n\n**é—®é¢˜**ï¼š\n\n- ç‰¹å¾è®¾è®¡ä¾èµ–ä¸“å®¶çŸ¥è¯†\n- æ³›åŒ–èƒ½åŠ›æœ‰é™\n\n#### æ·±åº¦å­¦ä¹ ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\n\n**ç»å…¸æ¶æ„æ¼”è¿›**ï¼š\n\n```\nAlexNet (2012) â†’ VGGNet (2014) â†’ ResNet (2015) â†’ EfficientNet (2019)\n```\n\n**æ ¸å¿ƒæ€æƒ³**ï¼š\n\né€šè¿‡å¤šå±‚å·ç§¯+æ± åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ ä»ä½å±‚ï¼ˆè¾¹ç¼˜ï¼‰åˆ°é«˜å±‚ï¼ˆç‰©ä½“ï¼‰çš„ç‰¹å¾å±‚æ¬¡ã€‚\n\n**ç¤ºä¾‹ï¼šResNet-50**\n\n```python\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nmodel.eval()\n\n# æå–ç‰¹å¾ï¼ˆå»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼‰\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# è¾“å…¥å›¾åƒ â†’ 2048ç»´ç‰¹å¾å‘é‡\nimage_tensor = preprocess(image)  # (3, 224, 224)\nfeatures = feature_extractor(image_tensor.unsqueeze(0))  # (1, 2048, 1, 1)\nfeatures = features.squeeze()  # (2048,)\n```\n\n#### æœ€æ–°ï¼šè§†è§‰Transformerï¼ˆViTï¼‰\n\n**æ€æƒ³**ï¼š\n\nå°†å›¾åƒåˆ‡åˆ†ä¸º patchesï¼Œç”¨ Transformer å¤„ç†ï¼ˆç±»ä¼¼ NLP ä¸­çš„è¯ï¼‰ã€‚\n\n**ç¤ºä¾‹**ï¼š\n\n```python\nfrom transformers import ViTModel, ViTImageProcessor\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nimage_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n```\n\n#### è‡ªç›‘ç£å­¦ä¹ ï¼šCLIPï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰\n\n**æ ¸å¿ƒæ€æƒ³**ï¼ˆRadford et al., 2021ï¼‰ï¼š\n\nè”åˆè®­ç»ƒå›¾åƒç¼–ç å™¨ä¸æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹åœ¨åµŒå…¥ç©ºé—´ä¸­æ¥è¿‘ã€‚\n\n**ä¼˜åŠ¿**ï¼š\n\n- **é›¶æ ·æœ¬è¿ç§»**ï¼šæ— éœ€æ ‡æ³¨æ•°æ®å³å¯åˆ†ç±»\n- **è·¨æ¨¡æ€æ£€ç´¢**ï¼šç”¨æ–‡æœ¬æŸ¥è¯¢å›¾åƒï¼Œæˆ–åä¹‹\n\n**ç¤ºä¾‹**ï¼š\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# å›¾åƒ + æ–‡æœ¬\nimage = load_image(\"store.jpg\")\ntexts = [\"a busy retail store\", \"an empty store\"]\n\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\n\n# è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦\nlogits_per_image = outputs.logits_per_image  # (1, 2)\nprobs = logits_per_image.softmax(dim=1)  # [0.8, 0.2] â†’ \"busy\"\n```\n\n### é‡‘èåº”ç”¨æ¡ˆä¾‹\n\n#### å«æ˜Ÿå›¾åƒé¢„æµ‹ç»æµæ´»åŠ¨\n\n**ä»»åŠ¡**ï¼šç”¨åœè½¦åœºè½¦è¾†æ•°é‡é¢„æµ‹é›¶å”®å•†å®¢æµã€‚\n\n**æµç¨‹**ï¼š\n\n```\nå«æ˜Ÿå›¾åƒï¼ˆå•†åœºåœè½¦åœºï¼‰\n    â†“\nç›®æ ‡æ£€æµ‹ï¼ˆYOLO/Faster R-CNNï¼‰â†’ è½¦è¾†æ•°é‡\n    â†“\næ—¶é—´åºåˆ—èšåˆ â†’ å‘¨/æœˆå¹³å‡è½¦è¾†æ•°\n    â†“\nå¯¹é½åˆ°å…¬å¸ â†’ é¢„æµ‹é”€å”®é¢/è‚¡ä»·\n```\n\n**ç»å…¸æ–‡çŒ®**ï¼š\n\n- Naik et al. (2019): *Measuring Economic Activity from Space*\n\n**å…³é”®æŒ‘æˆ˜**ï¼š\n\n- **ç©ºé—´å¯¹é½**ï¼šåœè½¦åœº â†’ å…·ä½“é—¨åº— â†’ ä¸Šå¸‚å…¬å¸\n- **äº‘å±‚é®æŒ¡**ï¼šç¼ºå¤±æ•°æ®å¤„ç†\n- **å­£èŠ‚æ€§**ï¼šèŠ‚å‡æ—¥/å¤©æ°”å½±å“\n\n#### è´¢æŠ¥å›¾è¡¨è‡ªåŠ¨æå–\n\n**ä»»åŠ¡**ï¼šä»å¹´æŠ¥ PDF ä¸­æå–è¶‹åŠ¿å›¾ï¼Œè¿˜åŸæ•°æ®ã€‚\n\n**æµç¨‹**ï¼š\n\n```\nPDF â†’ å›¾åƒæå– â†’ å›¾è¡¨æ£€æµ‹ â†’ OCR æ•°å­— â†’ æ•°æ®é‡å»º\n```\n\n**å·¥å…·**ï¼š\n\n- **å›¾è¡¨æ£€æµ‹**ï¼šDetectron2\n- **OCR**ï¼šTesseract, PaddleOCR\n- **æ•°æ®æå–**ï¼šChartOCR (ä¸“ç”¨å·¥å…·)\n\n**é£é™©**ï¼š\n\n- **ä¿¡æ¯æ³„éœ²**ï¼šå¿…é¡»ç¡®è®¤å›¾è¡¨åœ¨é¢„æµ‹æ—¶ç‚¹å·²å‘å¸ƒ\n- **æµ‹é‡è¯¯å·®**ï¼šOCR è¯†åˆ«é”™è¯¯\n\n#### ç¤¾äº¤åª’ä½“å›¾ç‰‡åˆ†æ\n\n**ä»»åŠ¡**ï¼šå“ç‰Œæ›å…‰åº¦ç›‘æµ‹ã€‚\n\n**æµç¨‹**ï¼š\n\n```\nInstagram/Twitter å›¾ç‰‡\n    â†“\nå“ç‰Œ Logo æ£€æµ‹ï¼ˆYOLO å¾®è°ƒï¼‰\n    â†“\nèšåˆï¼šæ¯æ—¥å“ç‰Œæ›å…‰æ¬¡æ•°\n    â†“\né¢„æµ‹ï¼šå“ç‰Œä»·å€¼ã€è‚¡ä»·\n```\n\n**æŒ‘æˆ˜**ï¼š\n\n- **æ ·æœ¬é€‰æ‹©åè¯¯**ï¼šç¤¾äº¤åª’ä½“ç”¨æˆ·ä¸ä»£è¡¨æ•´ä½“äººç¾¤\n- **æ—¶é—´å¯¹é½**ï¼šå›¾ç‰‡å‘å¸ƒæ—¶é—´ vs å¸‚åœºååº”\n\n### å…³é”®å¯¹é½ä¸åè¯¯\n\n#### æ‹æ‘„æ—¥ â‰  æŠ«éœ²æ—¥\n\n**é—®é¢˜**ï¼š\n\n- å«æ˜Ÿå›¾åƒæ‹æ‘„äº $t$ æ—¥ï¼Œä½†ä¸‹è½½/å¤„ç†åæ‰åœ¨ $t+k$ æ—¥å¯ç”¨\n- è‹¥ç”¨ $t$ æ—¥å›¾åƒé¢„æµ‹ $t$ æ—¥æ”¶ç›Š â†’ **å‰ç»åè¯¯**\n\n**è§£å†³**ï¼š\n\n```python\n# è®°å½•å›¾åƒæ‹æ‘„æ—¶é—´ä¸è·å–æ—¶é—´\ndf['image_date'] = '2024-01-15'     # å«æ˜Ÿè¿‡å¢ƒæ—¶é—´\ndf['available_date'] = '2024-01-18' # å›¾åƒä¸‹è½½/å¤„ç†å®Œæˆæ—¶é—´\n\n# ç”¨ available_date å¯¹é½åˆ°äº¤æ˜“æ—¥\ndf['trade_date'] = df['available_date'].apply(next_trading_day)\n```\n\n#### ç©ºé—´å¯¹é½\n\n**é—®é¢˜**ï¼š\n\n- å«æ˜Ÿå›¾åƒè¦†ç›–åŒºåŸŸ â†’ å¤šä¸ªé—¨åº— â†’ å¤šä¸ªå…¬å¸\n\n**è§£å†³**ï¼š\n\n- ä½¿ç”¨**åœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰**ç²¾ç¡®åŒ¹é…\n- è®°å½•åŒ¹é…è§„åˆ™ä¸è¯¯å·®èŒƒå›´\n\n#### åŸŸæ¼‚ç§»ï¼ˆDomain Shiftï¼‰\n\n**é—®é¢˜**ï¼š\n\n- æ¨¡å‹åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒï¼ˆè‡ªç„¶å›¾åƒï¼‰\n- åº”ç”¨äºé‡‘èå›¾åƒï¼ˆå«æ˜Ÿã€ç¥¨æ®ï¼‰æ—¶æ€§èƒ½ä¸‹é™\n\n**è§£å†³**ï¼š\n\n- **å¾®è°ƒ**ï¼ˆFine-tuningï¼‰ï¼šåœ¨ç›®æ ‡åŸŸä¸Šç»§ç»­è®­ç»ƒ\n- **é¢†åŸŸè‡ªé€‚åº”**ï¼šå‡å°‘æºåŸŸä¸ç›®æ ‡åŸŸçš„åˆ†å¸ƒå·®å¼‚\n\n#### è´¨é‡æ§åˆ¶\n\n**å¸¸è§é—®é¢˜**ï¼š\n\n- äº‘å±‚é®æŒ¡ï¼ˆå«æ˜Ÿå›¾åƒï¼‰\n- å¤œé—´å›¾åƒè¿‡æš—\n- å‹ç¼©å¤±çœŸ\n\n**è§£å†³**ï¼š\n\n- å›¾åƒè´¨é‡è¯„åˆ†ï¼ˆæ¨¡ç³Šåº¦ã€äº®åº¦æ£€æµ‹ï¼‰\n- å‰”é™¤ä½è´¨é‡æ ·æœ¬\n\n## éŸ³é¢‘æ¨¡æ€ï¼šç‰¹å¾æå–ä¸æµ‹é‡è¯¯å·®\n\n### ä¸¤æ¡ç‰¹å¾æå–ç®¡çº¿\n\n#### ç®¡çº¿ 1ï¼šASR â†’ æ–‡æœ¬ï¼ˆæ¥ç¬¬3å‘¨æµç¨‹ï¼‰\n\n**Automatic Speech Recognitionï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰**\n\n**æµç¨‹**ï¼š\n\n```\néŸ³é¢‘æ–‡ä»¶ï¼ˆ.wav/.mp3ï¼‰\n    â†“\nASR æ¨¡å‹ï¼ˆWhisper/Google Cloud Speechï¼‰\n    â†“\næ–‡æœ¬è½¬å½•\n    â†“\nåº”ç”¨ç¬¬3å‘¨çš„æ–‡æœ¬åˆ†ææ–¹æ³•\n```\n\n**å·¥å…·**ï¼š\n\n```python\nimport whisper\n\n# OpenAI Whisperï¼ˆå¼€æºï¼Œé«˜ç²¾åº¦ï¼‰\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"earnings_call.mp3\")\ntext = result[\"text\"]\n```\n\n**ä¼˜ç‚¹**ï¼š\n\n- å¯ç”¨æˆç†Ÿçš„ NLP æ–¹æ³•\n- å¯è§£é‡Šæ€§å¼º\n\n**ç¼ºç‚¹**ï¼š\n\n- **ASR é”™è¯¯**ï¼šè¯†åˆ«ä¸å‡†ï¼ˆç‰¹åˆ«æ˜¯ä¸“ä¸šæœ¯è¯­ã€å£éŸ³ï¼‰\n- **ä¸¢å¤±éŸµå¾‹ä¿¡æ¯**ï¼šè¯­è°ƒã€åœé¡¿ã€è¯­é€Ÿ\n\n#### ç®¡çº¿ 2ï¼šç›´æ¥æå–å£°å­¦/éŸµå¾‹ç‰¹å¾\n\n**ä¸è½¬æ–‡æœ¬ï¼Œç›´æ¥ä»éŸ³é¢‘æå–ç‰¹å¾**ï¼š\n\n| ç‰¹å¾ç±»åˆ« | å…·ä½“ç‰¹å¾ | é‡‘èå«ä¹‰ |\n|---------|---------|---------|\n| **éŸµå¾‹ï¼ˆProsodyï¼‰** | éŸ³é«˜ï¼ˆpitchï¼‰ã€è¯­é€Ÿï¼ˆspeaking rateï¼‰ã€åœé¡¿ï¼ˆpauseï¼‰ | æƒ…ç»ªã€ç´§å¼ åº¦ |\n| **éŸ³è´¨ï¼ˆVoice qualityï¼‰** | é¢¤éŸ³ï¼ˆjitterï¼‰ã€æµŠéŸ³ï¼ˆshimmerï¼‰ | å‹åŠ›ã€ä¸ç¡®å®šæ€§ |\n| **èƒ½é‡** | éŸ³é‡ã€èƒ½é‡åˆ†å¸ƒ | å¼ºè°ƒã€ä¿¡å¿ƒ |\n| **ä½å±‚ç‰¹å¾** | MFCCï¼ˆæ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°ï¼‰ | é€šç”¨éŸ³é¢‘è¡¨ç¤º |\n| **æ·±åº¦åµŒå…¥** | Wav2Vec 2.0, HuBERT | ç«¯åˆ°ç«¯å­¦ä¹  |\n\n#### æå–ç¤ºä¾‹ï¼šéŸµå¾‹ç‰¹å¾\n\n```python\nimport librosa\nimport numpy as np\n\n# åŠ è½½éŸ³é¢‘\ny, sr = librosa.load(\"audio.wav\", sr=16000)\n\n# 1. éŸ³é«˜ï¼ˆåŸºé¢‘ï¼‰\npitches, magnitudes = librosa.piptrack(y=y, sr=sr)\npitch_mean = np.mean(pitches[pitches > 0])\n\n# 2. è¯­é€Ÿï¼ˆé€šè¿‡é›¶äº¤å‰ç‡ç²—ç•¥ä¼°è®¡ï¼‰\nzcr = librosa.feature.zero_crossing_rate(y)\nspeaking_rate = np.mean(zcr)\n\n# 3. åœé¡¿ï¼ˆæ£€æµ‹é™éŸ³æ®µï¼‰\nintervals = librosa.effects.split(y, top_db=20)\npause_count = len(intervals) - 1\n\nfeatures = {\n    'pitch_mean': pitch_mean,\n    'speaking_rate': speaking_rate,\n    'pause_count': pause_count,\n}\n```\n\n#### æå–ç¤ºä¾‹ï¼šæ·±åº¦åµŒå…¥ï¼ˆWav2Vec 2.0ï¼‰\n\n```python\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport torch\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n\n# éŸ³é¢‘ â†’ ç‰¹å¾å‘é‡\ninput_values = processor(y, sampling_rate=sr, return_tensors=\"pt\").input_values\nwith torch.no_grad():\n    hidden_states = model(input_values).last_hidden_state\n\n# æ—¶é—´å¹³å‡æ± åŒ– â†’ å›ºå®šé•¿åº¦å‘é‡\naudio_embedding = hidden_states.mean(dim=1)  # (1, 768)\n```\n\n### é‡‘èåº”ç”¨æ¡ˆä¾‹\n\n#### è´¢æŠ¥ç”µè¯ä¼šæƒ…ç»ªè¯†åˆ«\n\n**ä»»åŠ¡**ï¼šè¯†åˆ«ç®¡ç†å±‚çš„ä¸ç¡®å®šæ€§/ç´§å¼ åº¦ã€‚\n\n**ç‰¹å¾**ï¼š\n\n- **æ–‡æœ¬**ï¼šè¯å…¸æ³•ï¼ˆä¸ç¡®å®šæ€§è¯æ±‡ï¼‰\n- **éŸµå¾‹**ï¼šéŸ³é«˜æ–¹å·®ã€è¯­é€Ÿã€åœé¡¿é¢‘ç‡\n- **ç»“åˆ**ï¼šå¤šæ¨¡æ€èåˆ\n\n**ç»å…¸æ–‡çŒ®**ï¼š\n\n- Mayew & Venkatachalam (2012): *The Power of Voice*\n- Larcker & Zakolyukina (2012): *Detecting Deceptive Discussions*\n\n**å‘ç°**ï¼š\n\n- éŸ³é«˜å‡é«˜ã€åœé¡¿å¢å¤š â†’ ä¸ç¡®å®šæ€§é«˜\n- å¯¹æœªæ¥è‚¡ä»·æœ‰é¢„æµ‹åŠ›ï¼ˆç‹¬ç«‹äºæ–‡æœ¬ï¼‰\n\n#### å®¢æœå½•éŸ³è´¨é‡ç›‘æ§\n\n**ä»»åŠ¡**ï¼šè¯†åˆ«å®¢æˆ·ä¸æ»¡æƒ…ç»ªï¼Œé¢„è­¦æŠ•è¯‰ã€‚\n\n**ç‰¹å¾**ï¼š\n\n- å®¢æˆ·è¯­é€Ÿã€éŸ³é‡ï¼ˆæ€¥èºã€æ„¤æ€’ï¼‰\n- å®¢æœå“åº”æ—¶é—´\n\n#### äº¤æ˜“å‘˜é€šè¯åˆè§„ç›‘æ§\n\n**ä»»åŠ¡**ï¼šæ£€æµ‹å¼‚å¸¸è¡Œä¸ºï¼ˆå¦‚å†…å¹•äº¤æ˜“æš—ç¤ºï¼‰ã€‚\n\n**æŒ‘æˆ˜**ï¼š\n\n- å®æ—¶æ€§è¦æ±‚é«˜\n- æœ¯è¯­ä¸°å¯Œï¼ˆéœ€é¢†åŸŸ ASRï¼‰\n- éšç§åˆè§„\n\n### é£é™©ç‚¹ä¸æ³¨æ„äº‹é¡¹\n\n#### ASR é”™è¯¯ä¸ä¿¡é“å™ªå£°\n\n**ASR è¯é”™è¯¯ç‡ï¼ˆWERï¼‰**ï¼š\n\n$$\n\\text{WER} = \\frac{\\text{æ’å…¥} + \\text{åˆ é™¤} + \\text{æ›¿æ¢}}{\\text{æ€»è¯æ•°}}\n$$\n\n**å½±å“å› ç´ **ï¼š\n\n- å£éŸ³ã€è¯­é€Ÿ\n- èƒŒæ™¯å™ªå£°ï¼ˆç°åœºä¼šè®®ï¼‰\n- ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚å…¬å¸/äº§å“åï¼‰\n\n**ç¼“è§£**ï¼š\n\n- ä½¿ç”¨**é¢†åŸŸå¾®è°ƒ**çš„ ASRï¼ˆå¦‚é‡‘èä¸“ç”¨æ¨¡å‹ï¼‰\n- **è¯´è¯äººåˆ†ç¦»**ï¼šåŒºåˆ† CEO vs CFO vs åˆ†æå¸ˆ\n- **ç½®ä¿¡åº¦è¿‡æ»¤**ï¼šåˆ é™¤ä½ç½®ä¿¡åº¦è¯†åˆ«\n\n#### è¯´è¯äººåˆ†ç¦»ä¸èº«ä»½å¯¹é½\n\n**é—®é¢˜**ï¼š\n\nç”µè¯ä¼šä¸­å¤šäººå‘è¨€ï¼Œéœ€åŒºåˆ†è°è¯´äº†ä»€ä¹ˆã€‚\n\n**æŠ€æœ¯**ï¼š\n\n- **è¯´è¯äººåˆ†æ®µï¼ˆDiarizationï¼‰**ï¼šæ ‡è®°æ¯æ®µéŸ³é¢‘çš„è¯´è¯äºº\n  ```python\n  from pyannote.audio import Pipeline\n  \n  pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n  diarization = pipeline(\"audio.wav\")\n  \n  for turn, _, speaker in diarization.itertracks(yield_label=True):\n      print(f\"{speaker}: {turn.start}s - {turn.end}s\")\n  ```\n\n- **è¯´è¯äººè¯†åˆ«**ï¼šåŒ¹é…åˆ°å…·ä½“äººç‰©ï¼ˆCEO/CFOï¼‰\n\n**æŒ‘æˆ˜**ï¼š\n\n- éŸ³é¢‘è´¨é‡å·®æ—¶é”™è¯¯ç‡é«˜\n- èº«ä»½æ ‡æ³¨éœ€äººå·¥éªŒè¯\n\n#### å¯å¾—æ€§é€‰æ‹©åè¯¯\n\n**é—®é¢˜**ï¼š\n\nå¹¶éæ‰€æœ‰å…¬å¸éƒ½å…¬å¼€ç”µè¯ä¼šå½•éŸ³ã€‚\n\n- å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€\n- å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å…¬å¸å¯èƒ½ä¸å…¬å¼€\n\n**åæœ**ï¼š\n\n- æ ·æœ¬ä»£è¡¨æ€§å·®\n- æ¨¡å‹æ¨å¹¿åˆ°å…¨å¸‚åœºæ—¶å¤±æ•ˆ\n\n**ç¼“è§£**ï¼š\n\n- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾ï¼ˆå¸‚å€¼ã€è¡Œä¸šåˆ†å¸ƒï¼‰\n- å€¾å‘è¯„åˆ†åŠ æƒï¼ˆPropensity Score Weightingï¼‰\n\n## è§†é¢‘æ¨¡æ€ï¼šæ—¶åºèšåˆã€è¡Œä¸ºä¿¡å·ä¸åˆè§„è¾¹ç•Œ\n\n### è§†é¢‘ = å›¾åƒåºåˆ— + éŸ³é¢‘\n\n**è§†é¢‘ç‰¹å¾**ï¼š\n\n- **è§†è§‰é€šé“**ï¼šå¸§çº§å›¾åƒç‰¹å¾\n- **éŸ³é¢‘é€šé“**ï¼šè¯­éŸ³/èƒŒæ™¯éŸ³ç‰¹å¾\n- **æ—¶åºå»ºæ¨¡**ï¼šæ•æ‰åŠ¨æ€å˜åŒ–\n\n### å¸§çº§è§†è§‰è¡¨å¾\n\n#### æå–å•å¸§ç‰¹å¾\n\n```python\nimport cv2\n\n# è¯»å–è§†é¢‘\ncap = cv2.VideoCapture(\"video.mp4\")\n\nframes = []\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frames.append(frame)\n\ncap.release()\n\n# å¯¹æ¯å¸§æå– CNN ç‰¹å¾\nfrom torchvision.models import resnet50\nmodel = resnet50(pretrained=True)\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\nframe_features = []\nfor frame in frames[::30]:  # æ¯ç§’1å¸§ï¼ˆå‡è®¾30fpsï¼‰\n    tensor = preprocess(frame)\n    feature = feature_extractor(tensor.unsqueeze(0)).squeeze()\n    frame_features.append(feature.numpy())\n\n# å¾—åˆ° (T, 2048) çš„ç‰¹å¾çŸ©é˜µï¼ŒT = å¸§æ•°\n```\n\n#### æ—¶é—´èšåˆ\n\n**ç®€å•æ–¹æ³•**ï¼š\n\n- **å¹³å‡æ± åŒ–**ï¼š$\\mathbf{v}_{\\text{video}} = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{v}_t$\n- **æœ€å¤§æ± åŒ–**ï¼š$\\mathbf{v}_{\\text{video}} = \\max_{t=1}^T \\mathbf{v}_t$\n\n**é«˜çº§æ–¹æ³•**ï¼š\n\n- **æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNï¼‰**\n- **LSTM/GRU**ï¼šæ•æ‰é•¿æœŸä¾èµ–\n- **Transformer**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶\n\n### è¡Œä¸ºçº¿ç´¢ï¼šéè¨€è¯­ä¿¡å·\n\n#### é¢éƒ¨åŠ¨ä½œç¼–ç ï¼ˆFacial Action Units, AUsï¼‰\n\n**å®šä¹‰**ï¼š\n\nPaul Ekman æå‡ºçš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨ç¼–ç ç³»ç»Ÿï¼Œå…± 46 ä¸ª AUã€‚\n\n**ç¤ºä¾‹**ï¼š\n\n- AU1: å†…çœ‰ä¸Šæ‰¬ï¼ˆæƒŠè®¶ï¼‰\n- AU4: çš±çœ‰ï¼ˆæ²‰æ€ã€æ‹…å¿§ï¼‰\n- AU12: å˜´è§’ä¸Šæ‰¬ï¼ˆå¾®ç¬‘ï¼‰\n\n**æå–å·¥å…·**ï¼š\n\n```python\n# OpenFaceï¼ˆå¼€æºï¼‰\nimport subprocess\n\nsubprocess.run([\n    \"FeatureExtraction\",\n    \"-f\", \"video.mp4\",\n    \"-out_dir\", \"output/\"\n])\n\n# è¾“å‡º CSV æ–‡ä»¶ï¼ŒåŒ…å«æ¯å¸§çš„ AU å¼ºåº¦\nimport pandas as pd\naus = pd.read_csv(\"output/video.csv\")\nprint(aus[['AU01_r', 'AU04_r', 'AU12_r']].head())\n```\n\n**é‡‘èåº”ç”¨**ï¼š\n\n- CEO åœ¨è·¯æ¼”ä¸­çš„å¾®è¡¨æƒ…ï¼ˆç´§å¼ ã€è‡ªä¿¡ï¼‰\n- ä¸è‚¡ä»·/èèµ„æˆåŠŸç‡çš„å…³ç³»\n\n#### å¤´éƒ¨å§¿æ€ï¼ˆHead Poseï¼‰\n\n**ç‰¹å¾**ï¼š\n\n- Yawï¼ˆå·¦å³è½¬ï¼‰\n- Pitchï¼ˆä¸Šä¸‹ç‚¹å¤´ï¼‰\n- Rollï¼ˆå·¦å³å€¾æ–œï¼‰\n\n**å«ä¹‰**ï¼š\n\n- é¢‘ç¹å›é¿ç›®å…‰ â†’ ä¸è¯šå®ï¼Ÿ\n- é¢‘ç¹ç‚¹å¤´ â†’ è®¤åŒ/å¼ºè°ƒ\n\n#### æ³¨è§†ï¼ˆGazeï¼‰\n\n**ç‰¹å¾**ï¼š\n\n- çœ¼ç›æ³¨è§†æ–¹å‘\n- çœ¼ç¥æ¥è§¦æ—¶é•¿\n\n**é‡‘èåº”ç”¨**ï¼š\n\n- ç®¡ç†å±‚å›ç­”é—®é¢˜æ—¶çš„çœ¼ç¥å˜åŒ–\n\n### æ—¶åºå»ºæ¨¡ç¤ºä¾‹ï¼šLSTM\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass VideoClassifier(nn.Module):\n    def __init__(self, input_dim=2048, hidden_dim=512, num_classes=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n    \n    def forward(self, x):\n        # x: (batch, seq_len, input_dim)\n        _, (h_n, _) = self.lstm(x)\n        # h_n: (1, batch, hidden_dim)\n        out = self.fc(h_n.squeeze(0))\n        return out\n\n# è®­ç»ƒ\nmodel = VideoClassifier()\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    for video_features, labels in dataloader:\n        optimizer.zero_grad()\n        outputs = model(video_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n```\n\n### é£é™©ç‚¹ä¸åˆè§„è¾¹ç•Œ\n\n#### æµ‹é‡è¯¯å·®æ¥æº\n\n**è§’åº¦/å…‰ç…§/å‹ç¼©**ï¼š\n\n- ä¾§è„¸æ£€æµ‹ AU ä¸å‡†\n- é€†å…‰å¯¼è‡´é¢éƒ¨ä¸æ¸…æ™°\n- è§†é¢‘å‹ç¼©ä¸¢å¤±ç»†èŠ‚\n\n**é®æŒ¡**ï¼š\n\n- æ‰‹é®ä½è„¸\n- é¢å…·/å£ç½©\n\n**ç¼“è§£**ï¼š\n\n- è´¨é‡è¯„åˆ†ï¼šå‰”é™¤ä½è´¨é‡å¸§\n- æ•°æ®å¢å¼ºï¼šè®­ç»ƒæ—¶æ¨¡æ‹Ÿå„ç§æ¡ä»¶\n\n#### ç®—æ³•åå·®\n\n**é—®é¢˜**ï¼š\n\né¢éƒ¨è¯†åˆ«ç®—æ³•åœ¨ä¸åŒäººç§/æ€§åˆ«ä¸Šçš„å‡†ç¡®ç‡ä¸åŒã€‚\n\n- å¯¹ç™½äººã€ç”·æ€§çš„è¯†åˆ«ç‡æ›´é«˜\n- å¯¹æ·±è‰²çš®è‚¤ã€å¥³æ€§çš„è¯†åˆ«ç‡è¾ƒä½\n\n**åæœ**ï¼š\n\n- æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“çš„é¢„æµ‹å­˜åœ¨ç³»ç»Ÿæ€§åå·®\n- ä¼¦ç†ä¸å…¬å¹³æ€§é—®é¢˜\n\n**ç¼“è§£**ï¼š\n\n- ä½¿ç”¨**å…¬å¹³æ€§ä¼˜åŒ–**çš„æ¨¡å‹\n- æŠ¥å‘Šä¸åŒç¾¤ä½“çš„æ€§èƒ½å·®å¼‚\n- åœ¨æ•æ„Ÿåœºæ™¯ä¸­é¿å…ä½¿ç”¨\n\n#### æ¶‰åŠç”Ÿç‰©ç‰¹å¾çš„åˆè§„è¾¹ç•Œ\n\n:::{.callout-caution}\nâš ï¸ æ³•å¾‹ä¸ä¼¦ç†çº¢çº¿\n\n**ç”Ÿç‰©ç‰¹å¾æ•°æ®**ï¼ˆé¢éƒ¨ã€å£°çº¹ã€è™¹è†œç­‰ï¼‰å—ä¸¥æ ¼ç›‘ç®¡ï¼š\n\n**æ³•è§„**ï¼š\n\n- **æ¬§ç›Ÿ GDPR**ï¼šéœ€æ˜ç¡®åŒæ„ï¼Œç”¨é€”å—é™\n- **ä¸­å›½ã€Šä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‹**ï¼šæ•æ„Ÿä¸ªäººä¿¡æ¯ï¼Œéœ€å•ç‹¬åŒæ„\n- **ç¾å›½å„å·æ³•å¾‹**ï¼šå¦‚åŠ å· CCPAã€ä¼Šåˆ©è¯ºä¼Šå· BIPA\n\n**ç¦æ­¢åœºæ™¯**ï¼š\n\n- æœªç»åŒæ„çš„ç§˜å¯†æ”¶é›†\n- ç”¨äºæ­§è§†æ€§å†³ç­–ï¼ˆå¦‚è´·æ¬¾å®¡æ‰¹åŸºäºé¢ç›¸ï¼‰\n- å¤§è§„æ¨¡å…¬å…±ç›‘æ§\n\n**åˆè§„å»ºè®®**ï¼š\n\n1. **æ˜ç¡®å‘ŠçŸ¥**ï¼šæ•°æ®æ”¶é›†ç›®çš„ã€ç”¨é€”\n2. **è·å–åŒæ„**ï¼šå¯æ’¤å›çš„æ˜ç¡®æˆæƒ\n3. **æœ€å°åŒ–åŸåˆ™**ï¼šåªæ”¶é›†å¿…è¦æ•°æ®\n4. **å»æ ‡è¯†åŒ–**ï¼šå°½å¯èƒ½åŒ¿ååŒ–\n5. **äººå·¥ç›‘ç£**ï¼šæ•æ„Ÿå†³ç­–éœ€äººç±»å®¡æ ¸\n:::\n\n## èåˆä¸åˆ†ç±»è¯„ä¼°ï¼šå¤šæ¨¡æ€å¢é‡ä¿¡æ¯æ£€éªŒ\n\n### èåˆç­–ç•¥\n\n#### æ—©èåˆï¼ˆEarly Fusionï¼‰\n\n**åœ¨ç‰¹å¾å±‚é¢æ‹¼æ¥**ï¼š\n\n```python\n# å›¾åƒç‰¹å¾ (2048ç»´) + éŸ³é¢‘ç‰¹å¾ (768ç»´) + æ–‡æœ¬ç‰¹å¾ (768ç»´)\nmultimodal_feature = np.concatenate([\n    image_embedding,\n    audio_embedding,\n    text_embedding\n])  # (3584ç»´)\n\n# è¾“å…¥åˆ°åˆ†ç±»å™¨\nclassifier.fit(multimodal_feature, label)\n```\n\n**ä¼˜ç‚¹**ï¼šç®€å•\n\n**ç¼ºç‚¹**ï¼š\n\n- ä¸åŒæ¨¡æ€ç»´åº¦å·®å¼‚å¤§ï¼Œå¯èƒ½ä¸»å¯¼æ€§ä¸å‡\n- ç¼ºå¤±æ¨¡æ€æ—¶éš¾ä»¥å¤„ç†\n\n#### ä¸­æœŸèåˆï¼ˆMid Fusionï¼‰\n\n**å„æ¨¡æ€å…ˆå•ç‹¬ç¼–ç ï¼Œå†èåˆ**ï¼š\n\n```python\nclass MidFusionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.image_encoder = nn.Linear(2048, 512)\n        self.audio_encoder = nn.Linear(768, 512)\n        self.text_encoder = nn.Linear(768, 512)\n        self.fusion = nn.Linear(512*3, 256)\n        self.classifier = nn.Linear(256, 2)\n    \n    def forward(self, image, audio, text):\n        img_emb = self.image_encoder(image)\n        aud_emb = self.audio_encoder(audio)\n        txt_emb = self.text_encoder(text)\n        \n        fused = torch.cat([img_emb, aud_emb, txt_emb], dim=1)\n        fused = F.relu(self.fusion(fused))\n        return self.classifier(fused)\n```\n\n**ä¼˜ç‚¹**ï¼šå„æ¨¡æ€åœ°ä½å¹³ç­‰\n\n#### åèåˆï¼ˆLate Fusionï¼‰\n\n**å„æ¨¡æ€å•ç‹¬é¢„æµ‹ï¼Œå†é›†æˆ**ï¼š\n\n```python\n# è®­ç»ƒä¸‰ä¸ªç‹¬ç«‹åˆ†ç±»å™¨\nimage_pred = image_classifier.predict_proba(image_features)\naudio_pred = audio_classifier.predict_proba(audio_features)\ntext_pred = text_classifier.predict_proba(text_features)\n\n# åŠ æƒå¹³å‡\nfinal_pred = 0.4 * image_pred + 0.3 * audio_pred + 0.3 * text_pred\n```\n\n**ä¼˜ç‚¹**ï¼š\n\n- å¯è§£é‡Šæ€§å¼ºï¼ˆçŸ¥é“å„æ¨¡æ€è´¡çŒ®ï¼‰\n- å®¹æ˜“å¤„ç†ç¼ºå¤±æ¨¡æ€\n\n**ç¼ºç‚¹**ï¼š\n\n- æœªåˆ©ç”¨æ¨¡æ€é—´äº¤äº’\n\n#### æ³¨æ„åŠ›èåˆï¼ˆAttention-basedï¼‰\n\n**è‡ªåŠ¨å­¦ä¹ å„æ¨¡æ€çš„æƒé‡**ï¼š\n\n```python\nclass AttentionFusion(nn.Module):\n    def __init__(self, input_dim=512):\n        super().__init__()\n        self.attention = nn.Linear(input_dim, 1)\n    \n    def forward(self, modalities):\n        # modalities: list of (batch, input_dim)\n        stacked = torch.stack(modalities, dim=1)  # (batch, n_modalities, input_dim)\n        \n        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n        attn_scores = self.attention(stacked).squeeze(-1)  # (batch, n_modalities)\n        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)  # (batch, n_modalities, 1)\n        \n        # åŠ æƒå’Œ\n        fused = (stacked * attn_weights).sum(dim=1)  # (batch, input_dim)\n        return fused\n```\n\n**ä¼˜ç‚¹**ï¼šè‡ªé€‚åº”èåˆ\n\n### ç¼ºå¤±æ¨¡æ€å¤„ç†\n\n**ç°å®é—®é¢˜**ï¼š\n\n- éƒ¨åˆ†å…¬å¸æ²¡æœ‰ç”µè¯ä¼šå½•éŸ³ï¼ˆéŸ³é¢‘ç¼ºå¤±ï¼‰\n- éƒ¨åˆ†æ—¶æœŸå«æ˜Ÿå›¾åƒæœ‰äº‘é®æŒ¡ï¼ˆå›¾åƒç¼ºå¤±ï¼‰\n\n**ç­–ç•¥**ï¼š\n\n1. **åˆ é™¤ç¼ºå¤±æ ·æœ¬**ï¼ˆç®€å•ä½†æŸå¤±æ•°æ®ï¼‰\n2. **ç”¨é›¶å‘é‡å¡«å……**\n3. **è®­ç»ƒå•æ¨¡æ€ + å¤šæ¨¡æ€æ¨¡å‹**ï¼Œç¼ºå¤±æ—¶ç”¨å•æ¨¡æ€\n\n### å¢é‡è´¡çŒ®è¯„ä¼°\n\n#### å®éªŒè®¾è®¡\n\n**åŸºå‡†æ¨¡å‹**ï¼šä»…ç”¨ç»“æ„åŒ–å˜é‡ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰\n\n**å¢å¼ºæ¨¡å‹**ï¼š+ æ–‡æœ¬å˜é‡\n\n**å¤šæ¨¡æ€æ¨¡å‹**ï¼š+ æ–‡æœ¬ + å›¾åƒ/éŸ³é¢‘\n\n**æ¯”è¾ƒ**ï¼š\n\n| æ¨¡å‹ | AUC | F1 | ç›¸å¯¹æå‡ |\n|-----|-----|-----|---------|\n| åŸºå‡†ï¼ˆç»“æ„åŒ–ï¼‰ | 0.75 | 0.68 | - |\n| + æ–‡æœ¬ | 0.78 | 0.71 | +4% AUC |\n| + æ–‡æœ¬ + éŸ³é¢‘ | 0.80 | 0.73 | +6.7% AUC |\n\n**ç»Ÿè®¡æ£€éªŒ**ï¼š\n\n- **DeLong æ£€éªŒ**ï¼šæ¯”è¾ƒ AUC æ˜¯å¦æ˜¾è‘—ä¸åŒ\n- **é…å¯¹ t æ£€éªŒ**ï¼šæ¯”è¾ƒé¢„æµ‹è¯¯å·®\n\n#### äº’è¡¥æ€§æ£€éªŒ\n\n**é—®é¢˜**ï¼šå¤šæ¨¡æ€ä¿¡å·æ˜¯å¦åªæ˜¯é‡å¤æ–‡æœ¬ä¿¡æ¯ï¼Ÿ\n\n**æ£€éªŒ**ï¼š\n\n1. **ç›¸å…³æ€§åˆ†æ**\n   ```python\n   corr = df[['text_sentiment', 'audio_uncertainty', 'image_activity']].corr()\n   ```\n   - ç›¸å…³ç³»æ•° < 0.6 ä¸ºå¥åº·\n\n2. **åˆ†ç»„åˆ†æ**\n   - åœ¨æ–‡æœ¬ä¿¡å·å¼±çš„æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘/å›¾åƒæ˜¯å¦ä½œç”¨æ›´å¼ºï¼Ÿ\n\n3. **æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰**\n   - é€æ­¥ç§»é™¤å„æ¨¡æ€ï¼Œè§‚å¯Ÿæ€§èƒ½ä¸‹é™\n\n#### è¯¯å·®åˆ†æ\n\n**æ¡ˆä¾‹åˆ†æ**ï¼š\n\n- å“ªäº›æ ·æœ¬é¢„æµ‹é”™è¯¯ï¼Ÿ\n- æ˜¯å¦æŸç±»æ ·æœ¬ï¼ˆå¦‚å°å¸‚å€¼ï¼‰æ›´éš¾é¢„æµ‹ï¼Ÿ\n- å¤šæ¨¡æ€åœ¨å“ªäº›æƒ…å†µä¸‹å¸®åŠ©æœ€å¤§ï¼Ÿ\n\n### åˆ‡ç‰‡è¯„ä¼°ä¸ç¨³å¥æ€§\n\n**æ—¶é—´åˆ‡ç‰‡**ï¼š\n\n- è®­ç»ƒæœŸ vs æµ‹è¯•æœŸæ€§èƒ½å·®å¼‚\n- ä¸åŒå¸‚åœºçŠ¶æ€ï¼ˆç‰›å¸‚/ç†Šå¸‚ï¼‰\n\n**æ¨ªæˆªé¢åˆ‡ç‰‡**ï¼š\n\n- ä¸åŒè¡Œä¸šï¼ˆç§‘æŠ€ vs åˆ¶é€ ï¼‰\n- ä¸åŒè§„æ¨¡ï¼ˆå¤§å¸‚å€¼ vs å°å¸‚å€¼ï¼‰\n\n**ç›®çš„**ï¼šç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚\n\n## æœ¬å‘¨å°ç»“\n\n### æ ¸å¿ƒè¦ç‚¹\n\n1. **å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡**ï¼šæ˜ç¡®æ ‡ç­¾å®šä¹‰ï¼ˆä½•æ—¶ã€å¯¹è°ã€é¢„æµ‹ä»€ä¹ˆï¼‰\n2. **å›¾åƒç‰¹å¾**ï¼šCNN/ViT/CLIPï¼Œæ³¨æ„ç©ºé—´å¯¹é½ã€è´¨é‡æ§åˆ¶ã€åŸŸæ¼‚ç§»\n3. **éŸ³é¢‘ç‰¹å¾**ï¼šASR â†’ æ–‡æœ¬ vs å£°å­¦/éŸµå¾‹ç‰¹å¾ï¼Œæ³¨æ„ ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦»\n4. **è§†é¢‘ç‰¹å¾**ï¼šå¸§çº§ + æ—¶åºå»ºæ¨¡ï¼Œè¡Œä¸ºçº¿ç´¢ï¼ˆAU/å§¿æ€/æ³¨è§†ï¼‰ï¼Œæ³¨æ„æµ‹é‡è¯¯å·®\n5. **èåˆç­–ç•¥**ï¼šæ—©/ä¸­/åèåˆã€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤„ç†ç¼ºå¤±æ¨¡æ€\n6. **å¢é‡æ£€éªŒ**ï¼šç›¸å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡çš„å¢é‡è´¡çŒ®ï¼Œäº’è¡¥æ€§è¯æ®\n7. **ä¼¦ç†è¾¹ç•Œ**ï¼šç”Ÿç‰©ç‰¹å¾åˆè§„ã€ç®—æ³•åå·®ã€éšç§ä¿æŠ¤\n\n### æœ¬å‘¨æ€è€ƒé¢˜\n\n#### é—®é¢˜ 1\n\nä»¥\"ä¸šç»©ç”µè¯ä¼šéŸ³é¢‘\"æ„é€ ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚è¯†åˆ«ç®¡ç†å±‚ä¸ç¡®å®šæ€§çŠ¶æ€ï¼‰æ—¶ï¼Œæ ‡ç­¾é€šå¸¸å¦‚ä½•å®šä¹‰ï¼Ÿå“ªäº›ç¯èŠ‚æœ€å®¹æ˜“å¼•å…¥æ ·æœ¬é€‰æ‹©åè¯¯ï¼Ÿ\n\n:::{.callout-note collapse=\"true\"}\nğŸ’¡ å‚è€ƒç­”æ¡ˆ\n\n**æ ‡ç­¾å®šä¹‰**ï¼š\n\n1. **åŸºäºæœªæ¥è‚¡ä»·**ï¼š\n   - $y = \\mathbb{I}(r_{t+1 \\to t+k} < \\text{å¸‚åœºä¸­ä½æ•°})$\n   - ä¸ç¡®å®šæ€§é«˜ â†’ æœªæ¥è¡¨ç°å·®\n\n2. **åŸºäºè´¢åŠ¡æ„å¤–**ï¼š\n   - $y = \\mathbb{I}(|\\text{å®é™…EPS} - \\text{é¢„æœŸEPS}| > \\text{é˜ˆå€¼})$\n   - ä¸ç¡®å®šæ€§é«˜ â†’ é¢„æµ‹è¯¯å·®å¤§\n\n3. **åŸºäºæ–‡æœ¬è¯å…¸**ï¼ˆè¾…åŠ©æ ‡æ³¨ï¼‰ï¼š\n   - ç»Ÿè®¡ä¸ç¡®å®šæ€§è¯æ±‡ï¼ˆ\"uncertain\", \"might\", \"could\"ï¼‰\n   - äººå·¥éªŒè¯éƒ¨åˆ†æ ·æœ¬\n\n**æ ·æœ¬é€‰æ‹©åè¯¯æ¥æº**ï¼š\n\n1. **å¯å¾—æ€§åè¯¯**\n   - å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€å½•éŸ³\n   - å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å¯èƒ½éšè—\n\n2. **å­˜æ´»åè¯¯**\n   - é€€å¸‚å…¬å¸çš„ç”µè¯ä¼šæ•°æ®ç¼ºå¤±\n\n3. **æ—¶æœŸé€‰æ‹©**\n   - å±æœºæœŸé—´ç”µè¯ä¼šå–æ¶ˆç‡æ›´é«˜\n\n**ç¼“è§£**ï¼š\n\n- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾åˆ†å¸ƒ\n- å€¾å‘è¯„åˆ†åŠ æƒ\n- ç¨³å¥æ€§æ£€éªŒï¼ˆä»…ç”¨æŒç»­æœ‰æ•°æ®çš„å…¬å¸ï¼‰\n:::\n\n#### é—®é¢˜ 2\n\nå¦‚ä½•ä¸¥è°¨åœ°è¯æ˜å¤šæ¨¡æ€ä¿¡å·å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡å…·æœ‰å¢é‡ä¿¡æ¯ï¼Ÿè¯·ç»™å‡ºä¸€ä¸ªä½ è®¤ä¸ºå¿…è¦çš„äº’è¡¥æ€§æ£€éªŒè®¾è®¡ã€‚\n\n:::{.callout-note collapse=\"true\"}\nğŸ’¡ å‚è€ƒç­”æ¡ˆ\n\n**å®Œæ•´æ£€éªŒæµç¨‹**ï¼š\n\n**1. åŸºå‡†å¯¹æ¯”**\n\n| æ¨¡å‹ | ç‰¹å¾ | AUC | $\\Delta$ AUC |\n|-----|------|-----|-------------|\n| M1 | ç»“æ„åŒ–ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰ | 0.72 | - |\n| M2 | M1 + æ–‡æœ¬ | 0.76 | +0.04 |\n| M3 | M2 + éŸ³é¢‘ | 0.79 | +0.03 |\n\n- **æ¡ä»¶**ï¼š$\\Delta$ AUC ç»Ÿè®¡æ˜¾è‘—ï¼ˆDeLong æ£€éªŒ $p < 0.05$ï¼‰\n\n**2. ç›¸å…³æ€§åˆ†æ**\n\n```python\ncorr_matrix = df[['structured', 'text', 'audio']].corr()\nassert corr_matrix.loc['audio', 'text'] < 0.6  # é¿å…é«˜åº¦å†—ä½™\n```\n\n**3. æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ**\n\nåœ¨æ§åˆ¶æ–‡æœ¬åï¼ŒéŸ³é¢‘æ˜¯å¦ä»æœ‰é¢„æµ‹åŠ›ï¼Ÿ\n\n$$\n\\text{Logit}(y) = \\beta_0 + \\beta_1 \\text{Text} + \\beta_2 \\text{Audio} + \\varepsilon\n$$\n\n- æ£€éªŒï¼š$\\beta_2$ æ˜¯å¦æ˜¾è‘—ï¼Ÿ\n\n**4. åˆ†ç»„å¼‚è´¨æ€§**\n\nåœ¨æ–‡æœ¬ä¿¡å·å¼±çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘è´¡çŒ®æ›´å¤§ï¼š\n\n```\nåˆ†ç»„ï¼šæ–‡æœ¬æƒ…æ„Ÿ = ä¸­æ€§ï¼ˆä¿¡æ¯é‡å°‘ï¼‰\n  â†’ éŸ³é¢‘ AUC æå‡æ›´æ˜¾è‘—\n```\n\n**5. æ¶ˆèå®éªŒ**\n\n| ç§»é™¤æ¨¡æ€ | AUC ä¸‹é™ | è§£é‡Š |\n|---------|---------|-----|\n| å»é™¤éŸ³é¢‘ | -0.03 | éŸ³é¢‘æœ‰ç‹¬ç‰¹è´¡çŒ® |\n| å»é™¤æ–‡æœ¬ | -0.04 | æ–‡æœ¬ä»é‡è¦ |\n| å»é™¤ç»“æ„åŒ– | -0.05 | åŸºç¡€å˜é‡æœ€é‡è¦ |\n\n**ç»“è®ºæ¨¡æ¿**ï¼š\n\n\"éŸ³é¢‘ç‰¹å¾åœ¨æ§åˆ¶æ–‡æœ¬ä¸ç»“æ„åŒ–å˜é‡åï¼Œä»èƒ½å¸¦æ¥ç»Ÿè®¡æ˜¾è‘—çš„ AUC æå‡ï¼ˆ+0.03, p=0.01ï¼‰ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºéŸ³é¢‘ä¸æ–‡æœ¬ç›¸å…³ç³»æ•°ä¸º 0.45ï¼Œè¡¨æ˜äºŒè€…æ•æ‰äº†éƒ¨åˆ†é‡å ä½†éå®Œå…¨å†—ä½™çš„ä¿¡æ¯ã€‚åœ¨æ–‡æœ¬æƒ…æ„Ÿä¸­æ€§çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘çš„å¢é‡è´¡çŒ®æ›´å¤§ï¼ˆAUC +0.05ï¼‰ï¼Œè¯å®äº†äº’è¡¥æ€§ã€‚\"\n:::\n\n---\n\n### è¯¾ç¨‹å‰åŠæ®µå›é¡¾\n\nå‰å››å‘¨æˆ‘ä»¬å»ºç«‹äº†**AI èµ‹èƒ½é‡‘èç ”ç©¶çš„å®Œæ•´æ¡†æ¶**ï¼š\n\n- **ç¬¬1å‘¨**ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆi.i.d. åœºæ™¯ï¼‰\n- **ç¬¬2å‘¨**ï¼šé‡‘èæ—¶åºè¯„ä¼°ä¸å›æµ‹è§„èŒƒ\n- **ç¬¬3å‘¨**ï¼šæ–‡æœ¬åˆ†æçš„å¯å®¡è®¡æµç¨‹\n- **ç¬¬4å‘¨**ï¼šå¤šæ¨¡æ€ä¿¡å·çš„å¢é‡æ£€éªŒ\n\n**å…±åŒä¸»çº¿**ï¼š\n\n1. **æ³›åŒ–èƒ½åŠ›**ï¼šæ ·æœ¬å¤–è¡¨ç°æ˜¯å”¯ä¸€æ ‡å‡†\n2. **å¯å®¡è®¡æ€§**ï¼šä»åŸå§‹æ•°æ®åˆ°å˜é‡çš„æ¯ä¸€æ­¥å¯è¿½æº¯\n3. **å¢é‡ä»·å€¼**ï¼šæ–°æ–¹æ³•å¿…é¡»ç›¸å¯¹å·²çŸ¥æ–¹æ³•å±•ç¤ºå¢é‡ä¿¡æ¯\n\n### ååŠæ®µè¯¾ç¨‹é¢„å‘Š\n\n**ç¬¬5-8å‘¨**ï¼šå­¦ç”Ÿç ”ç©¶è®¡åˆ’æ±‡æŠ¥ä¸ç ”è®¨\n\n- æ¯ç»„å±•ç¤ºç ”ç©¶é—®é¢˜ã€æ•°æ®ã€æ–¹æ³•ã€è¯„ä¼°åè®®\n- è¯¾å ‚è®¨è®ºä¸åé¦ˆ\n- å¼ºåŒ–**æœ€å°è¯æ®é“¾**æ„è¯†\n\n**ç¬¬5å‘¨å‰**å°†æœ‰**è¯¾å‰æµ‹éªŒ**ï¼Œè¦†ç›–ç¬¬1-4å‘¨æ ¸å¿ƒæ¦‚å¿µï¼Œè¯·æå‰å¤ä¹ ï¼\n\n---\n\n**ç¥å­¦ä¹ é¡ºåˆ©ï¼æœŸå¾…çœ‹åˆ°ä½ ä»¬çš„ç²¾å½©ç ”ç©¶ï¼**\n","srcMarkdownNoYaml":"\n\n## ä»»åŠ¡ä¸æ•°æ®ï¼šæŠŠå¤šæ¨¡æ€ç ”ç©¶è¡¨è¿°ä¸ºåˆ†ç±»é—®é¢˜\n\n### ä¸ºä»€ä¹ˆæ˜¯\"åˆ†ç±»\"ï¼Ÿ\n\nè™½ç„¶å¤šæ¨¡æ€æ•°æ®å½¢å¼å¤šæ ·ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ï¼Œä½†åœ¨é‡‘èåº”ç”¨ä¸­ï¼Œæ ¸å¿ƒä»»åŠ¡é€šå¸¸å¯è¡¨è¿°ä¸º**åˆ†ç±»é—®é¢˜**ï¼š\n\n- é¢„æµ‹**æ¶¨/è·Œ**ï¼ˆäºŒåˆ†ç±»ï¼‰\n- è¯†åˆ«**ç®¡ç†å±‚æƒ…ç»ªçŠ¶æ€**ï¼ˆå¤šåˆ†ç±»ï¼šä¹è§‚/ä¸­æ€§/æ‚²è§‚ï¼‰\n- åˆ¤æ–­**æ¬ºè¯ˆ/éæ¬ºè¯ˆ**ï¼ˆäºŒåˆ†ç±»ï¼‰\n- é¢„æµ‹**ä¿¡ç”¨ç­‰çº§**ï¼ˆå¤šåˆ†ç±»ï¼‰\n\n:::{.callout-note icon=false}\nğŸ¯ æ ‡ç­¾å®šä¹‰çš„ä¸‰è¦ç´ \n\n1. **ä½•æ—¶**ï¼ˆWhenï¼‰ï¼šé¢„æµ‹å“ªä¸ªæ—¶æœŸçš„ç»“æœï¼Ÿ\n2. **å¯¹è°**ï¼ˆWhoï¼‰ï¼šä¸ªè‚¡/è¡Œä¸š/å¸‚åœºï¼Ÿ\n3. **é¢„æµ‹ä»€ä¹ˆ**ï¼ˆWhatï¼‰ï¼šå…·ä½“çš„ç±»åˆ«å®šä¹‰ï¼Ÿ\n\n**ç¤ºä¾‹**ï¼š\n\n- **ä½•æ—¶**ï¼šä¸šç»©ç”µè¯ä¼šåç¬¬äºŒä¸ªäº¤æ˜“æ—¥\n- **å¯¹è°**ï¼šè¯¥å…¬å¸è‚¡ç¥¨\n- **é¢„æµ‹ä»€ä¹ˆ**ï¼šæ”¶ç›Šç‡æ˜¯å¦è¶…è¿‡å¸‚åœºä¸­ä½æ•°ï¼ˆäºŒåˆ†ç±»ï¼‰\n\næ ‡ç­¾å®šä¹‰å†³å®šäº†ï¼š\n\n- æ•°æ®å¯¹é½æ–¹å¼ï¼ˆä½•æ—¶æå–ç‰¹å¾ï¼‰\n- è¯„ä¼°åè®®ï¼ˆæ—¶åº CV çš„çª—å£è®¾ç½®ï¼‰\n- æ ·æœ¬é€‰æ‹©ï¼ˆæ˜¯å¦åŒ…å«åœç‰Œè‚¡ç¥¨ï¼‰\n:::\n\n### å¤šæ¨¡æ€æ•°æ®ç‰ˆå›¾\n\n#### å›¾åƒï¼ˆImageï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **å«æ˜Ÿå›¾åƒ** | é¢„æµ‹é›¶å”®å®¢æµã€å·¥å‚æ´»åŠ¨ã€å•†å“åº“å­˜ | ç©ºé—´å¯¹é½ã€äº‘å±‚é®æŒ¡ |\n| **å¤œå…‰æ•°æ®** | åŒºåŸŸç»æµæ´»åŠ¨å¼ºåº¦ | åˆ†è¾¨ç‡ä½ã€å­£èŠ‚æ€§ |\n| **ç¥¨æ®/åˆåŒæ‰«æä»¶** | OCR æå–ä¿¡æ¯ã€æ¬ºè¯ˆæ£€æµ‹ | ç‰ˆå¼å¤šæ ·ã€å™ªå£°å¤§ |\n| **è´¢æŠ¥å›¾è¡¨æˆªå›¾** | è‡ªåŠ¨æå–æ•°æ®è¶‹åŠ¿ | ä¿¡æ¯æ³„éœ²é£é™©ï¼ˆéœ€ç¡®è®¤å‘å¸ƒæ—¶é—´ï¼‰|\n| **ç¤¾äº¤åª’ä½“å›¾ç‰‡** | å“ç‰Œå½¢è±¡ç›‘æµ‹ã€äº§å“è¯†åˆ« | æ ·æœ¬é€‰æ‹©åè¯¯ |\n\n#### éŸ³é¢‘ï¼ˆAudioï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **è´¢æŠ¥ç”µè¯ä¼š** | ç®¡ç†å±‚æƒ…ç»ªã€ä¸ç¡®å®šæ€§è¯†åˆ« | ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦» |\n| **å®¢æœå½•éŸ³** | å®¢æˆ·æ»¡æ„åº¦ã€æŠ•è¯‰é¢„è­¦ | éšç§åˆè§„ã€ä¿¡é“å™ªå£° |\n| **äº¤æ˜“å‘˜é€šè¯** | åˆè§„ç›‘æ§ã€å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ | å®æ—¶æ€§è¦æ±‚ã€æœ¯è¯­è¯†åˆ« |\n\n#### è§†é¢‘ï¼ˆVideoï¼‰\n\n| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |\n|-------|---------|-----|\n| **ç®¡ç†å±‚è·¯æ¼”/å‘å¸ƒä¼š** | éè¨€è¯­çº¿ç´¢ï¼ˆé¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿ï¼‰| è§’åº¦/å…‰ç…§å˜åŒ–ã€é®æŒ¡ |\n| **ç›‘æ§è§†é¢‘** | ç½‘ç‚¹å®¢æµã€ATM å¼‚å¸¸ | è®¡ç®—æˆæœ¬é«˜ã€éšç§é—®é¢˜ |\n\n## è§†è§‰ï¼ˆå›¾åƒï¼‰æ¨¡æ€ï¼šç‰¹å¾æå–ä¸å˜é‡æ„é€ \n\n### å›¾åƒè¡¨ç¤ºæ–¹æ³•æ¼”è¿›\n\n#### ä¼ ç»Ÿæ–¹æ³•ï¼šæ‰‹å·¥ç‰¹å¾\n\n**ä½å±‚ç‰¹å¾**ï¼š\n\n- é¢œè‰²ç›´æ–¹å›¾\n- çº¹ç†ï¼ˆGabor æ»¤æ³¢å™¨ã€LBPï¼‰\n- è¾¹ç¼˜æ£€æµ‹ï¼ˆCannyã€Sobelï¼‰\n\n**ä¸­å±‚ç‰¹å¾**ï¼š\n\n- SIFTï¼ˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢ï¼‰\n- HOGï¼ˆæ–¹å‘æ¢¯åº¦ç›´æ–¹å›¾ï¼‰\n\n**é—®é¢˜**ï¼š\n\n- ç‰¹å¾è®¾è®¡ä¾èµ–ä¸“å®¶çŸ¥è¯†\n- æ³›åŒ–èƒ½åŠ›æœ‰é™\n\n#### æ·±åº¦å­¦ä¹ ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\n\n**ç»å…¸æ¶æ„æ¼”è¿›**ï¼š\n\n```\nAlexNet (2012) â†’ VGGNet (2014) â†’ ResNet (2015) â†’ EfficientNet (2019)\n```\n\n**æ ¸å¿ƒæ€æƒ³**ï¼š\n\né€šè¿‡å¤šå±‚å·ç§¯+æ± åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ ä»ä½å±‚ï¼ˆè¾¹ç¼˜ï¼‰åˆ°é«˜å±‚ï¼ˆç‰©ä½“ï¼‰çš„ç‰¹å¾å±‚æ¬¡ã€‚\n\n**ç¤ºä¾‹ï¼šResNet-50**\n\n```python\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nmodel.eval()\n\n# æå–ç‰¹å¾ï¼ˆå»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼‰\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# è¾“å…¥å›¾åƒ â†’ 2048ç»´ç‰¹å¾å‘é‡\nimage_tensor = preprocess(image)  # (3, 224, 224)\nfeatures = feature_extractor(image_tensor.unsqueeze(0))  # (1, 2048, 1, 1)\nfeatures = features.squeeze()  # (2048,)\n```\n\n#### æœ€æ–°ï¼šè§†è§‰Transformerï¼ˆViTï¼‰\n\n**æ€æƒ³**ï¼š\n\nå°†å›¾åƒåˆ‡åˆ†ä¸º patchesï¼Œç”¨ Transformer å¤„ç†ï¼ˆç±»ä¼¼ NLP ä¸­çš„è¯ï¼‰ã€‚\n\n**ç¤ºä¾‹**ï¼š\n\n```python\nfrom transformers import ViTModel, ViTImageProcessor\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nimage_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n```\n\n#### è‡ªç›‘ç£å­¦ä¹ ï¼šCLIPï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰\n\n**æ ¸å¿ƒæ€æƒ³**ï¼ˆRadford et al., 2021ï¼‰ï¼š\n\nè”åˆè®­ç»ƒå›¾åƒç¼–ç å™¨ä¸æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹åœ¨åµŒå…¥ç©ºé—´ä¸­æ¥è¿‘ã€‚\n\n**ä¼˜åŠ¿**ï¼š\n\n- **é›¶æ ·æœ¬è¿ç§»**ï¼šæ— éœ€æ ‡æ³¨æ•°æ®å³å¯åˆ†ç±»\n- **è·¨æ¨¡æ€æ£€ç´¢**ï¼šç”¨æ–‡æœ¬æŸ¥è¯¢å›¾åƒï¼Œæˆ–åä¹‹\n\n**ç¤ºä¾‹**ï¼š\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# å›¾åƒ + æ–‡æœ¬\nimage = load_image(\"store.jpg\")\ntexts = [\"a busy retail store\", \"an empty store\"]\n\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\n\n# è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦\nlogits_per_image = outputs.logits_per_image  # (1, 2)\nprobs = logits_per_image.softmax(dim=1)  # [0.8, 0.2] â†’ \"busy\"\n```\n\n### é‡‘èåº”ç”¨æ¡ˆä¾‹\n\n#### å«æ˜Ÿå›¾åƒé¢„æµ‹ç»æµæ´»åŠ¨\n\n**ä»»åŠ¡**ï¼šç”¨åœè½¦åœºè½¦è¾†æ•°é‡é¢„æµ‹é›¶å”®å•†å®¢æµã€‚\n\n**æµç¨‹**ï¼š\n\n```\nå«æ˜Ÿå›¾åƒï¼ˆå•†åœºåœè½¦åœºï¼‰\n    â†“\nç›®æ ‡æ£€æµ‹ï¼ˆYOLO/Faster R-CNNï¼‰â†’ è½¦è¾†æ•°é‡\n    â†“\næ—¶é—´åºåˆ—èšåˆ â†’ å‘¨/æœˆå¹³å‡è½¦è¾†æ•°\n    â†“\nå¯¹é½åˆ°å…¬å¸ â†’ é¢„æµ‹é”€å”®é¢/è‚¡ä»·\n```\n\n**ç»å…¸æ–‡çŒ®**ï¼š\n\n- Naik et al. (2019): *Measuring Economic Activity from Space*\n\n**å…³é”®æŒ‘æˆ˜**ï¼š\n\n- **ç©ºé—´å¯¹é½**ï¼šåœè½¦åœº â†’ å…·ä½“é—¨åº— â†’ ä¸Šå¸‚å…¬å¸\n- **äº‘å±‚é®æŒ¡**ï¼šç¼ºå¤±æ•°æ®å¤„ç†\n- **å­£èŠ‚æ€§**ï¼šèŠ‚å‡æ—¥/å¤©æ°”å½±å“\n\n#### è´¢æŠ¥å›¾è¡¨è‡ªåŠ¨æå–\n\n**ä»»åŠ¡**ï¼šä»å¹´æŠ¥ PDF ä¸­æå–è¶‹åŠ¿å›¾ï¼Œè¿˜åŸæ•°æ®ã€‚\n\n**æµç¨‹**ï¼š\n\n```\nPDF â†’ å›¾åƒæå– â†’ å›¾è¡¨æ£€æµ‹ â†’ OCR æ•°å­— â†’ æ•°æ®é‡å»º\n```\n\n**å·¥å…·**ï¼š\n\n- **å›¾è¡¨æ£€æµ‹**ï¼šDetectron2\n- **OCR**ï¼šTesseract, PaddleOCR\n- **æ•°æ®æå–**ï¼šChartOCR (ä¸“ç”¨å·¥å…·)\n\n**é£é™©**ï¼š\n\n- **ä¿¡æ¯æ³„éœ²**ï¼šå¿…é¡»ç¡®è®¤å›¾è¡¨åœ¨é¢„æµ‹æ—¶ç‚¹å·²å‘å¸ƒ\n- **æµ‹é‡è¯¯å·®**ï¼šOCR è¯†åˆ«é”™è¯¯\n\n#### ç¤¾äº¤åª’ä½“å›¾ç‰‡åˆ†æ\n\n**ä»»åŠ¡**ï¼šå“ç‰Œæ›å…‰åº¦ç›‘æµ‹ã€‚\n\n**æµç¨‹**ï¼š\n\n```\nInstagram/Twitter å›¾ç‰‡\n    â†“\nå“ç‰Œ Logo æ£€æµ‹ï¼ˆYOLO å¾®è°ƒï¼‰\n    â†“\nèšåˆï¼šæ¯æ—¥å“ç‰Œæ›å…‰æ¬¡æ•°\n    â†“\né¢„æµ‹ï¼šå“ç‰Œä»·å€¼ã€è‚¡ä»·\n```\n\n**æŒ‘æˆ˜**ï¼š\n\n- **æ ·æœ¬é€‰æ‹©åè¯¯**ï¼šç¤¾äº¤åª’ä½“ç”¨æˆ·ä¸ä»£è¡¨æ•´ä½“äººç¾¤\n- **æ—¶é—´å¯¹é½**ï¼šå›¾ç‰‡å‘å¸ƒæ—¶é—´ vs å¸‚åœºååº”\n\n### å…³é”®å¯¹é½ä¸åè¯¯\n\n#### æ‹æ‘„æ—¥ â‰  æŠ«éœ²æ—¥\n\n**é—®é¢˜**ï¼š\n\n- å«æ˜Ÿå›¾åƒæ‹æ‘„äº $t$ æ—¥ï¼Œä½†ä¸‹è½½/å¤„ç†åæ‰åœ¨ $t+k$ æ—¥å¯ç”¨\n- è‹¥ç”¨ $t$ æ—¥å›¾åƒé¢„æµ‹ $t$ æ—¥æ”¶ç›Š â†’ **å‰ç»åè¯¯**\n\n**è§£å†³**ï¼š\n\n```python\n# è®°å½•å›¾åƒæ‹æ‘„æ—¶é—´ä¸è·å–æ—¶é—´\ndf['image_date'] = '2024-01-15'     # å«æ˜Ÿè¿‡å¢ƒæ—¶é—´\ndf['available_date'] = '2024-01-18' # å›¾åƒä¸‹è½½/å¤„ç†å®Œæˆæ—¶é—´\n\n# ç”¨ available_date å¯¹é½åˆ°äº¤æ˜“æ—¥\ndf['trade_date'] = df['available_date'].apply(next_trading_day)\n```\n\n#### ç©ºé—´å¯¹é½\n\n**é—®é¢˜**ï¼š\n\n- å«æ˜Ÿå›¾åƒè¦†ç›–åŒºåŸŸ â†’ å¤šä¸ªé—¨åº— â†’ å¤šä¸ªå…¬å¸\n\n**è§£å†³**ï¼š\n\n- ä½¿ç”¨**åœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰**ç²¾ç¡®åŒ¹é…\n- è®°å½•åŒ¹é…è§„åˆ™ä¸è¯¯å·®èŒƒå›´\n\n#### åŸŸæ¼‚ç§»ï¼ˆDomain Shiftï¼‰\n\n**é—®é¢˜**ï¼š\n\n- æ¨¡å‹åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒï¼ˆè‡ªç„¶å›¾åƒï¼‰\n- åº”ç”¨äºé‡‘èå›¾åƒï¼ˆå«æ˜Ÿã€ç¥¨æ®ï¼‰æ—¶æ€§èƒ½ä¸‹é™\n\n**è§£å†³**ï¼š\n\n- **å¾®è°ƒ**ï¼ˆFine-tuningï¼‰ï¼šåœ¨ç›®æ ‡åŸŸä¸Šç»§ç»­è®­ç»ƒ\n- **é¢†åŸŸè‡ªé€‚åº”**ï¼šå‡å°‘æºåŸŸä¸ç›®æ ‡åŸŸçš„åˆ†å¸ƒå·®å¼‚\n\n#### è´¨é‡æ§åˆ¶\n\n**å¸¸è§é—®é¢˜**ï¼š\n\n- äº‘å±‚é®æŒ¡ï¼ˆå«æ˜Ÿå›¾åƒï¼‰\n- å¤œé—´å›¾åƒè¿‡æš—\n- å‹ç¼©å¤±çœŸ\n\n**è§£å†³**ï¼š\n\n- å›¾åƒè´¨é‡è¯„åˆ†ï¼ˆæ¨¡ç³Šåº¦ã€äº®åº¦æ£€æµ‹ï¼‰\n- å‰”é™¤ä½è´¨é‡æ ·æœ¬\n\n## éŸ³é¢‘æ¨¡æ€ï¼šç‰¹å¾æå–ä¸æµ‹é‡è¯¯å·®\n\n### ä¸¤æ¡ç‰¹å¾æå–ç®¡çº¿\n\n#### ç®¡çº¿ 1ï¼šASR â†’ æ–‡æœ¬ï¼ˆæ¥ç¬¬3å‘¨æµç¨‹ï¼‰\n\n**Automatic Speech Recognitionï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰**\n\n**æµç¨‹**ï¼š\n\n```\néŸ³é¢‘æ–‡ä»¶ï¼ˆ.wav/.mp3ï¼‰\n    â†“\nASR æ¨¡å‹ï¼ˆWhisper/Google Cloud Speechï¼‰\n    â†“\næ–‡æœ¬è½¬å½•\n    â†“\nåº”ç”¨ç¬¬3å‘¨çš„æ–‡æœ¬åˆ†ææ–¹æ³•\n```\n\n**å·¥å…·**ï¼š\n\n```python\nimport whisper\n\n# OpenAI Whisperï¼ˆå¼€æºï¼Œé«˜ç²¾åº¦ï¼‰\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"earnings_call.mp3\")\ntext = result[\"text\"]\n```\n\n**ä¼˜ç‚¹**ï¼š\n\n- å¯ç”¨æˆç†Ÿçš„ NLP æ–¹æ³•\n- å¯è§£é‡Šæ€§å¼º\n\n**ç¼ºç‚¹**ï¼š\n\n- **ASR é”™è¯¯**ï¼šè¯†åˆ«ä¸å‡†ï¼ˆç‰¹åˆ«æ˜¯ä¸“ä¸šæœ¯è¯­ã€å£éŸ³ï¼‰\n- **ä¸¢å¤±éŸµå¾‹ä¿¡æ¯**ï¼šè¯­è°ƒã€åœé¡¿ã€è¯­é€Ÿ\n\n#### ç®¡çº¿ 2ï¼šç›´æ¥æå–å£°å­¦/éŸµå¾‹ç‰¹å¾\n\n**ä¸è½¬æ–‡æœ¬ï¼Œç›´æ¥ä»éŸ³é¢‘æå–ç‰¹å¾**ï¼š\n\n| ç‰¹å¾ç±»åˆ« | å…·ä½“ç‰¹å¾ | é‡‘èå«ä¹‰ |\n|---------|---------|---------|\n| **éŸµå¾‹ï¼ˆProsodyï¼‰** | éŸ³é«˜ï¼ˆpitchï¼‰ã€è¯­é€Ÿï¼ˆspeaking rateï¼‰ã€åœé¡¿ï¼ˆpauseï¼‰ | æƒ…ç»ªã€ç´§å¼ åº¦ |\n| **éŸ³è´¨ï¼ˆVoice qualityï¼‰** | é¢¤éŸ³ï¼ˆjitterï¼‰ã€æµŠéŸ³ï¼ˆshimmerï¼‰ | å‹åŠ›ã€ä¸ç¡®å®šæ€§ |\n| **èƒ½é‡** | éŸ³é‡ã€èƒ½é‡åˆ†å¸ƒ | å¼ºè°ƒã€ä¿¡å¿ƒ |\n| **ä½å±‚ç‰¹å¾** | MFCCï¼ˆæ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°ï¼‰ | é€šç”¨éŸ³é¢‘è¡¨ç¤º |\n| **æ·±åº¦åµŒå…¥** | Wav2Vec 2.0, HuBERT | ç«¯åˆ°ç«¯å­¦ä¹  |\n\n#### æå–ç¤ºä¾‹ï¼šéŸµå¾‹ç‰¹å¾\n\n```python\nimport librosa\nimport numpy as np\n\n# åŠ è½½éŸ³é¢‘\ny, sr = librosa.load(\"audio.wav\", sr=16000)\n\n# 1. éŸ³é«˜ï¼ˆåŸºé¢‘ï¼‰\npitches, magnitudes = librosa.piptrack(y=y, sr=sr)\npitch_mean = np.mean(pitches[pitches > 0])\n\n# 2. è¯­é€Ÿï¼ˆé€šè¿‡é›¶äº¤å‰ç‡ç²—ç•¥ä¼°è®¡ï¼‰\nzcr = librosa.feature.zero_crossing_rate(y)\nspeaking_rate = np.mean(zcr)\n\n# 3. åœé¡¿ï¼ˆæ£€æµ‹é™éŸ³æ®µï¼‰\nintervals = librosa.effects.split(y, top_db=20)\npause_count = len(intervals) - 1\n\nfeatures = {\n    'pitch_mean': pitch_mean,\n    'speaking_rate': speaking_rate,\n    'pause_count': pause_count,\n}\n```\n\n#### æå–ç¤ºä¾‹ï¼šæ·±åº¦åµŒå…¥ï¼ˆWav2Vec 2.0ï¼‰\n\n```python\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport torch\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n\n# éŸ³é¢‘ â†’ ç‰¹å¾å‘é‡\ninput_values = processor(y, sampling_rate=sr, return_tensors=\"pt\").input_values\nwith torch.no_grad():\n    hidden_states = model(input_values).last_hidden_state\n\n# æ—¶é—´å¹³å‡æ± åŒ– â†’ å›ºå®šé•¿åº¦å‘é‡\naudio_embedding = hidden_states.mean(dim=1)  # (1, 768)\n```\n\n### é‡‘èåº”ç”¨æ¡ˆä¾‹\n\n#### è´¢æŠ¥ç”µè¯ä¼šæƒ…ç»ªè¯†åˆ«\n\n**ä»»åŠ¡**ï¼šè¯†åˆ«ç®¡ç†å±‚çš„ä¸ç¡®å®šæ€§/ç´§å¼ åº¦ã€‚\n\n**ç‰¹å¾**ï¼š\n\n- **æ–‡æœ¬**ï¼šè¯å…¸æ³•ï¼ˆä¸ç¡®å®šæ€§è¯æ±‡ï¼‰\n- **éŸµå¾‹**ï¼šéŸ³é«˜æ–¹å·®ã€è¯­é€Ÿã€åœé¡¿é¢‘ç‡\n- **ç»“åˆ**ï¼šå¤šæ¨¡æ€èåˆ\n\n**ç»å…¸æ–‡çŒ®**ï¼š\n\n- Mayew & Venkatachalam (2012): *The Power of Voice*\n- Larcker & Zakolyukina (2012): *Detecting Deceptive Discussions*\n\n**å‘ç°**ï¼š\n\n- éŸ³é«˜å‡é«˜ã€åœé¡¿å¢å¤š â†’ ä¸ç¡®å®šæ€§é«˜\n- å¯¹æœªæ¥è‚¡ä»·æœ‰é¢„æµ‹åŠ›ï¼ˆç‹¬ç«‹äºæ–‡æœ¬ï¼‰\n\n#### å®¢æœå½•éŸ³è´¨é‡ç›‘æ§\n\n**ä»»åŠ¡**ï¼šè¯†åˆ«å®¢æˆ·ä¸æ»¡æƒ…ç»ªï¼Œé¢„è­¦æŠ•è¯‰ã€‚\n\n**ç‰¹å¾**ï¼š\n\n- å®¢æˆ·è¯­é€Ÿã€éŸ³é‡ï¼ˆæ€¥èºã€æ„¤æ€’ï¼‰\n- å®¢æœå“åº”æ—¶é—´\n\n#### äº¤æ˜“å‘˜é€šè¯åˆè§„ç›‘æ§\n\n**ä»»åŠ¡**ï¼šæ£€æµ‹å¼‚å¸¸è¡Œä¸ºï¼ˆå¦‚å†…å¹•äº¤æ˜“æš—ç¤ºï¼‰ã€‚\n\n**æŒ‘æˆ˜**ï¼š\n\n- å®æ—¶æ€§è¦æ±‚é«˜\n- æœ¯è¯­ä¸°å¯Œï¼ˆéœ€é¢†åŸŸ ASRï¼‰\n- éšç§åˆè§„\n\n### é£é™©ç‚¹ä¸æ³¨æ„äº‹é¡¹\n\n#### ASR é”™è¯¯ä¸ä¿¡é“å™ªå£°\n\n**ASR è¯é”™è¯¯ç‡ï¼ˆWERï¼‰**ï¼š\n\n$$\n\\text{WER} = \\frac{\\text{æ’å…¥} + \\text{åˆ é™¤} + \\text{æ›¿æ¢}}{\\text{æ€»è¯æ•°}}\n$$\n\n**å½±å“å› ç´ **ï¼š\n\n- å£éŸ³ã€è¯­é€Ÿ\n- èƒŒæ™¯å™ªå£°ï¼ˆç°åœºä¼šè®®ï¼‰\n- ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚å…¬å¸/äº§å“åï¼‰\n\n**ç¼“è§£**ï¼š\n\n- ä½¿ç”¨**é¢†åŸŸå¾®è°ƒ**çš„ ASRï¼ˆå¦‚é‡‘èä¸“ç”¨æ¨¡å‹ï¼‰\n- **è¯´è¯äººåˆ†ç¦»**ï¼šåŒºåˆ† CEO vs CFO vs åˆ†æå¸ˆ\n- **ç½®ä¿¡åº¦è¿‡æ»¤**ï¼šåˆ é™¤ä½ç½®ä¿¡åº¦è¯†åˆ«\n\n#### è¯´è¯äººåˆ†ç¦»ä¸èº«ä»½å¯¹é½\n\n**é—®é¢˜**ï¼š\n\nç”µè¯ä¼šä¸­å¤šäººå‘è¨€ï¼Œéœ€åŒºåˆ†è°è¯´äº†ä»€ä¹ˆã€‚\n\n**æŠ€æœ¯**ï¼š\n\n- **è¯´è¯äººåˆ†æ®µï¼ˆDiarizationï¼‰**ï¼šæ ‡è®°æ¯æ®µéŸ³é¢‘çš„è¯´è¯äºº\n  ```python\n  from pyannote.audio import Pipeline\n  \n  pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n  diarization = pipeline(\"audio.wav\")\n  \n  for turn, _, speaker in diarization.itertracks(yield_label=True):\n      print(f\"{speaker}: {turn.start}s - {turn.end}s\")\n  ```\n\n- **è¯´è¯äººè¯†åˆ«**ï¼šåŒ¹é…åˆ°å…·ä½“äººç‰©ï¼ˆCEO/CFOï¼‰\n\n**æŒ‘æˆ˜**ï¼š\n\n- éŸ³é¢‘è´¨é‡å·®æ—¶é”™è¯¯ç‡é«˜\n- èº«ä»½æ ‡æ³¨éœ€äººå·¥éªŒè¯\n\n#### å¯å¾—æ€§é€‰æ‹©åè¯¯\n\n**é—®é¢˜**ï¼š\n\nå¹¶éæ‰€æœ‰å…¬å¸éƒ½å…¬å¼€ç”µè¯ä¼šå½•éŸ³ã€‚\n\n- å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€\n- å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å…¬å¸å¯èƒ½ä¸å…¬å¼€\n\n**åæœ**ï¼š\n\n- æ ·æœ¬ä»£è¡¨æ€§å·®\n- æ¨¡å‹æ¨å¹¿åˆ°å…¨å¸‚åœºæ—¶å¤±æ•ˆ\n\n**ç¼“è§£**ï¼š\n\n- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾ï¼ˆå¸‚å€¼ã€è¡Œä¸šåˆ†å¸ƒï¼‰\n- å€¾å‘è¯„åˆ†åŠ æƒï¼ˆPropensity Score Weightingï¼‰\n\n## è§†é¢‘æ¨¡æ€ï¼šæ—¶åºèšåˆã€è¡Œä¸ºä¿¡å·ä¸åˆè§„è¾¹ç•Œ\n\n### è§†é¢‘ = å›¾åƒåºåˆ— + éŸ³é¢‘\n\n**è§†é¢‘ç‰¹å¾**ï¼š\n\n- **è§†è§‰é€šé“**ï¼šå¸§çº§å›¾åƒç‰¹å¾\n- **éŸ³é¢‘é€šé“**ï¼šè¯­éŸ³/èƒŒæ™¯éŸ³ç‰¹å¾\n- **æ—¶åºå»ºæ¨¡**ï¼šæ•æ‰åŠ¨æ€å˜åŒ–\n\n### å¸§çº§è§†è§‰è¡¨å¾\n\n#### æå–å•å¸§ç‰¹å¾\n\n```python\nimport cv2\n\n# è¯»å–è§†é¢‘\ncap = cv2.VideoCapture(\"video.mp4\")\n\nframes = []\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frames.append(frame)\n\ncap.release()\n\n# å¯¹æ¯å¸§æå– CNN ç‰¹å¾\nfrom torchvision.models import resnet50\nmodel = resnet50(pretrained=True)\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\nframe_features = []\nfor frame in frames[::30]:  # æ¯ç§’1å¸§ï¼ˆå‡è®¾30fpsï¼‰\n    tensor = preprocess(frame)\n    feature = feature_extractor(tensor.unsqueeze(0)).squeeze()\n    frame_features.append(feature.numpy())\n\n# å¾—åˆ° (T, 2048) çš„ç‰¹å¾çŸ©é˜µï¼ŒT = å¸§æ•°\n```\n\n#### æ—¶é—´èšåˆ\n\n**ç®€å•æ–¹æ³•**ï¼š\n\n- **å¹³å‡æ± åŒ–**ï¼š$\\mathbf{v}_{\\text{video}} = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{v}_t$\n- **æœ€å¤§æ± åŒ–**ï¼š$\\mathbf{v}_{\\text{video}} = \\max_{t=1}^T \\mathbf{v}_t$\n\n**é«˜çº§æ–¹æ³•**ï¼š\n\n- **æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNï¼‰**\n- **LSTM/GRU**ï¼šæ•æ‰é•¿æœŸä¾èµ–\n- **Transformer**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶\n\n### è¡Œä¸ºçº¿ç´¢ï¼šéè¨€è¯­ä¿¡å·\n\n#### é¢éƒ¨åŠ¨ä½œç¼–ç ï¼ˆFacial Action Units, AUsï¼‰\n\n**å®šä¹‰**ï¼š\n\nPaul Ekman æå‡ºçš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨ç¼–ç ç³»ç»Ÿï¼Œå…± 46 ä¸ª AUã€‚\n\n**ç¤ºä¾‹**ï¼š\n\n- AU1: å†…çœ‰ä¸Šæ‰¬ï¼ˆæƒŠè®¶ï¼‰\n- AU4: çš±çœ‰ï¼ˆæ²‰æ€ã€æ‹…å¿§ï¼‰\n- AU12: å˜´è§’ä¸Šæ‰¬ï¼ˆå¾®ç¬‘ï¼‰\n\n**æå–å·¥å…·**ï¼š\n\n```python\n# OpenFaceï¼ˆå¼€æºï¼‰\nimport subprocess\n\nsubprocess.run([\n    \"FeatureExtraction\",\n    \"-f\", \"video.mp4\",\n    \"-out_dir\", \"output/\"\n])\n\n# è¾“å‡º CSV æ–‡ä»¶ï¼ŒåŒ…å«æ¯å¸§çš„ AU å¼ºåº¦\nimport pandas as pd\naus = pd.read_csv(\"output/video.csv\")\nprint(aus[['AU01_r', 'AU04_r', 'AU12_r']].head())\n```\n\n**é‡‘èåº”ç”¨**ï¼š\n\n- CEO åœ¨è·¯æ¼”ä¸­çš„å¾®è¡¨æƒ…ï¼ˆç´§å¼ ã€è‡ªä¿¡ï¼‰\n- ä¸è‚¡ä»·/èèµ„æˆåŠŸç‡çš„å…³ç³»\n\n#### å¤´éƒ¨å§¿æ€ï¼ˆHead Poseï¼‰\n\n**ç‰¹å¾**ï¼š\n\n- Yawï¼ˆå·¦å³è½¬ï¼‰\n- Pitchï¼ˆä¸Šä¸‹ç‚¹å¤´ï¼‰\n- Rollï¼ˆå·¦å³å€¾æ–œï¼‰\n\n**å«ä¹‰**ï¼š\n\n- é¢‘ç¹å›é¿ç›®å…‰ â†’ ä¸è¯šå®ï¼Ÿ\n- é¢‘ç¹ç‚¹å¤´ â†’ è®¤åŒ/å¼ºè°ƒ\n\n#### æ³¨è§†ï¼ˆGazeï¼‰\n\n**ç‰¹å¾**ï¼š\n\n- çœ¼ç›æ³¨è§†æ–¹å‘\n- çœ¼ç¥æ¥è§¦æ—¶é•¿\n\n**é‡‘èåº”ç”¨**ï¼š\n\n- ç®¡ç†å±‚å›ç­”é—®é¢˜æ—¶çš„çœ¼ç¥å˜åŒ–\n\n### æ—¶åºå»ºæ¨¡ç¤ºä¾‹ï¼šLSTM\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass VideoClassifier(nn.Module):\n    def __init__(self, input_dim=2048, hidden_dim=512, num_classes=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n    \n    def forward(self, x):\n        # x: (batch, seq_len, input_dim)\n        _, (h_n, _) = self.lstm(x)\n        # h_n: (1, batch, hidden_dim)\n        out = self.fc(h_n.squeeze(0))\n        return out\n\n# è®­ç»ƒ\nmodel = VideoClassifier()\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    for video_features, labels in dataloader:\n        optimizer.zero_grad()\n        outputs = model(video_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n```\n\n### é£é™©ç‚¹ä¸åˆè§„è¾¹ç•Œ\n\n#### æµ‹é‡è¯¯å·®æ¥æº\n\n**è§’åº¦/å…‰ç…§/å‹ç¼©**ï¼š\n\n- ä¾§è„¸æ£€æµ‹ AU ä¸å‡†\n- é€†å…‰å¯¼è‡´é¢éƒ¨ä¸æ¸…æ™°\n- è§†é¢‘å‹ç¼©ä¸¢å¤±ç»†èŠ‚\n\n**é®æŒ¡**ï¼š\n\n- æ‰‹é®ä½è„¸\n- é¢å…·/å£ç½©\n\n**ç¼“è§£**ï¼š\n\n- è´¨é‡è¯„åˆ†ï¼šå‰”é™¤ä½è´¨é‡å¸§\n- æ•°æ®å¢å¼ºï¼šè®­ç»ƒæ—¶æ¨¡æ‹Ÿå„ç§æ¡ä»¶\n\n#### ç®—æ³•åå·®\n\n**é—®é¢˜**ï¼š\n\né¢éƒ¨è¯†åˆ«ç®—æ³•åœ¨ä¸åŒäººç§/æ€§åˆ«ä¸Šçš„å‡†ç¡®ç‡ä¸åŒã€‚\n\n- å¯¹ç™½äººã€ç”·æ€§çš„è¯†åˆ«ç‡æ›´é«˜\n- å¯¹æ·±è‰²çš®è‚¤ã€å¥³æ€§çš„è¯†åˆ«ç‡è¾ƒä½\n\n**åæœ**ï¼š\n\n- æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“çš„é¢„æµ‹å­˜åœ¨ç³»ç»Ÿæ€§åå·®\n- ä¼¦ç†ä¸å…¬å¹³æ€§é—®é¢˜\n\n**ç¼“è§£**ï¼š\n\n- ä½¿ç”¨**å…¬å¹³æ€§ä¼˜åŒ–**çš„æ¨¡å‹\n- æŠ¥å‘Šä¸åŒç¾¤ä½“çš„æ€§èƒ½å·®å¼‚\n- åœ¨æ•æ„Ÿåœºæ™¯ä¸­é¿å…ä½¿ç”¨\n\n#### æ¶‰åŠç”Ÿç‰©ç‰¹å¾çš„åˆè§„è¾¹ç•Œ\n\n:::{.callout-caution}\nâš ï¸ æ³•å¾‹ä¸ä¼¦ç†çº¢çº¿\n\n**ç”Ÿç‰©ç‰¹å¾æ•°æ®**ï¼ˆé¢éƒ¨ã€å£°çº¹ã€è™¹è†œç­‰ï¼‰å—ä¸¥æ ¼ç›‘ç®¡ï¼š\n\n**æ³•è§„**ï¼š\n\n- **æ¬§ç›Ÿ GDPR**ï¼šéœ€æ˜ç¡®åŒæ„ï¼Œç”¨é€”å—é™\n- **ä¸­å›½ã€Šä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‹**ï¼šæ•æ„Ÿä¸ªäººä¿¡æ¯ï¼Œéœ€å•ç‹¬åŒæ„\n- **ç¾å›½å„å·æ³•å¾‹**ï¼šå¦‚åŠ å· CCPAã€ä¼Šåˆ©è¯ºä¼Šå· BIPA\n\n**ç¦æ­¢åœºæ™¯**ï¼š\n\n- æœªç»åŒæ„çš„ç§˜å¯†æ”¶é›†\n- ç”¨äºæ­§è§†æ€§å†³ç­–ï¼ˆå¦‚è´·æ¬¾å®¡æ‰¹åŸºäºé¢ç›¸ï¼‰\n- å¤§è§„æ¨¡å…¬å…±ç›‘æ§\n\n**åˆè§„å»ºè®®**ï¼š\n\n1. **æ˜ç¡®å‘ŠçŸ¥**ï¼šæ•°æ®æ”¶é›†ç›®çš„ã€ç”¨é€”\n2. **è·å–åŒæ„**ï¼šå¯æ’¤å›çš„æ˜ç¡®æˆæƒ\n3. **æœ€å°åŒ–åŸåˆ™**ï¼šåªæ”¶é›†å¿…è¦æ•°æ®\n4. **å»æ ‡è¯†åŒ–**ï¼šå°½å¯èƒ½åŒ¿ååŒ–\n5. **äººå·¥ç›‘ç£**ï¼šæ•æ„Ÿå†³ç­–éœ€äººç±»å®¡æ ¸\n:::\n\n## èåˆä¸åˆ†ç±»è¯„ä¼°ï¼šå¤šæ¨¡æ€å¢é‡ä¿¡æ¯æ£€éªŒ\n\n### èåˆç­–ç•¥\n\n#### æ—©èåˆï¼ˆEarly Fusionï¼‰\n\n**åœ¨ç‰¹å¾å±‚é¢æ‹¼æ¥**ï¼š\n\n```python\n# å›¾åƒç‰¹å¾ (2048ç»´) + éŸ³é¢‘ç‰¹å¾ (768ç»´) + æ–‡æœ¬ç‰¹å¾ (768ç»´)\nmultimodal_feature = np.concatenate([\n    image_embedding,\n    audio_embedding,\n    text_embedding\n])  # (3584ç»´)\n\n# è¾“å…¥åˆ°åˆ†ç±»å™¨\nclassifier.fit(multimodal_feature, label)\n```\n\n**ä¼˜ç‚¹**ï¼šç®€å•\n\n**ç¼ºç‚¹**ï¼š\n\n- ä¸åŒæ¨¡æ€ç»´åº¦å·®å¼‚å¤§ï¼Œå¯èƒ½ä¸»å¯¼æ€§ä¸å‡\n- ç¼ºå¤±æ¨¡æ€æ—¶éš¾ä»¥å¤„ç†\n\n#### ä¸­æœŸèåˆï¼ˆMid Fusionï¼‰\n\n**å„æ¨¡æ€å…ˆå•ç‹¬ç¼–ç ï¼Œå†èåˆ**ï¼š\n\n```python\nclass MidFusionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.image_encoder = nn.Linear(2048, 512)\n        self.audio_encoder = nn.Linear(768, 512)\n        self.text_encoder = nn.Linear(768, 512)\n        self.fusion = nn.Linear(512*3, 256)\n        self.classifier = nn.Linear(256, 2)\n    \n    def forward(self, image, audio, text):\n        img_emb = self.image_encoder(image)\n        aud_emb = self.audio_encoder(audio)\n        txt_emb = self.text_encoder(text)\n        \n        fused = torch.cat([img_emb, aud_emb, txt_emb], dim=1)\n        fused = F.relu(self.fusion(fused))\n        return self.classifier(fused)\n```\n\n**ä¼˜ç‚¹**ï¼šå„æ¨¡æ€åœ°ä½å¹³ç­‰\n\n#### åèåˆï¼ˆLate Fusionï¼‰\n\n**å„æ¨¡æ€å•ç‹¬é¢„æµ‹ï¼Œå†é›†æˆ**ï¼š\n\n```python\n# è®­ç»ƒä¸‰ä¸ªç‹¬ç«‹åˆ†ç±»å™¨\nimage_pred = image_classifier.predict_proba(image_features)\naudio_pred = audio_classifier.predict_proba(audio_features)\ntext_pred = text_classifier.predict_proba(text_features)\n\n# åŠ æƒå¹³å‡\nfinal_pred = 0.4 * image_pred + 0.3 * audio_pred + 0.3 * text_pred\n```\n\n**ä¼˜ç‚¹**ï¼š\n\n- å¯è§£é‡Šæ€§å¼ºï¼ˆçŸ¥é“å„æ¨¡æ€è´¡çŒ®ï¼‰\n- å®¹æ˜“å¤„ç†ç¼ºå¤±æ¨¡æ€\n\n**ç¼ºç‚¹**ï¼š\n\n- æœªåˆ©ç”¨æ¨¡æ€é—´äº¤äº’\n\n#### æ³¨æ„åŠ›èåˆï¼ˆAttention-basedï¼‰\n\n**è‡ªåŠ¨å­¦ä¹ å„æ¨¡æ€çš„æƒé‡**ï¼š\n\n```python\nclass AttentionFusion(nn.Module):\n    def __init__(self, input_dim=512):\n        super().__init__()\n        self.attention = nn.Linear(input_dim, 1)\n    \n    def forward(self, modalities):\n        # modalities: list of (batch, input_dim)\n        stacked = torch.stack(modalities, dim=1)  # (batch, n_modalities, input_dim)\n        \n        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n        attn_scores = self.attention(stacked).squeeze(-1)  # (batch, n_modalities)\n        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)  # (batch, n_modalities, 1)\n        \n        # åŠ æƒå’Œ\n        fused = (stacked * attn_weights).sum(dim=1)  # (batch, input_dim)\n        return fused\n```\n\n**ä¼˜ç‚¹**ï¼šè‡ªé€‚åº”èåˆ\n\n### ç¼ºå¤±æ¨¡æ€å¤„ç†\n\n**ç°å®é—®é¢˜**ï¼š\n\n- éƒ¨åˆ†å…¬å¸æ²¡æœ‰ç”µè¯ä¼šå½•éŸ³ï¼ˆéŸ³é¢‘ç¼ºå¤±ï¼‰\n- éƒ¨åˆ†æ—¶æœŸå«æ˜Ÿå›¾åƒæœ‰äº‘é®æŒ¡ï¼ˆå›¾åƒç¼ºå¤±ï¼‰\n\n**ç­–ç•¥**ï¼š\n\n1. **åˆ é™¤ç¼ºå¤±æ ·æœ¬**ï¼ˆç®€å•ä½†æŸå¤±æ•°æ®ï¼‰\n2. **ç”¨é›¶å‘é‡å¡«å……**\n3. **è®­ç»ƒå•æ¨¡æ€ + å¤šæ¨¡æ€æ¨¡å‹**ï¼Œç¼ºå¤±æ—¶ç”¨å•æ¨¡æ€\n\n### å¢é‡è´¡çŒ®è¯„ä¼°\n\n#### å®éªŒè®¾è®¡\n\n**åŸºå‡†æ¨¡å‹**ï¼šä»…ç”¨ç»“æ„åŒ–å˜é‡ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰\n\n**å¢å¼ºæ¨¡å‹**ï¼š+ æ–‡æœ¬å˜é‡\n\n**å¤šæ¨¡æ€æ¨¡å‹**ï¼š+ æ–‡æœ¬ + å›¾åƒ/éŸ³é¢‘\n\n**æ¯”è¾ƒ**ï¼š\n\n| æ¨¡å‹ | AUC | F1 | ç›¸å¯¹æå‡ |\n|-----|-----|-----|---------|\n| åŸºå‡†ï¼ˆç»“æ„åŒ–ï¼‰ | 0.75 | 0.68 | - |\n| + æ–‡æœ¬ | 0.78 | 0.71 | +4% AUC |\n| + æ–‡æœ¬ + éŸ³é¢‘ | 0.80 | 0.73 | +6.7% AUC |\n\n**ç»Ÿè®¡æ£€éªŒ**ï¼š\n\n- **DeLong æ£€éªŒ**ï¼šæ¯”è¾ƒ AUC æ˜¯å¦æ˜¾è‘—ä¸åŒ\n- **é…å¯¹ t æ£€éªŒ**ï¼šæ¯”è¾ƒé¢„æµ‹è¯¯å·®\n\n#### äº’è¡¥æ€§æ£€éªŒ\n\n**é—®é¢˜**ï¼šå¤šæ¨¡æ€ä¿¡å·æ˜¯å¦åªæ˜¯é‡å¤æ–‡æœ¬ä¿¡æ¯ï¼Ÿ\n\n**æ£€éªŒ**ï¼š\n\n1. **ç›¸å…³æ€§åˆ†æ**\n   ```python\n   corr = df[['text_sentiment', 'audio_uncertainty', 'image_activity']].corr()\n   ```\n   - ç›¸å…³ç³»æ•° < 0.6 ä¸ºå¥åº·\n\n2. **åˆ†ç»„åˆ†æ**\n   - åœ¨æ–‡æœ¬ä¿¡å·å¼±çš„æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘/å›¾åƒæ˜¯å¦ä½œç”¨æ›´å¼ºï¼Ÿ\n\n3. **æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰**\n   - é€æ­¥ç§»é™¤å„æ¨¡æ€ï¼Œè§‚å¯Ÿæ€§èƒ½ä¸‹é™\n\n#### è¯¯å·®åˆ†æ\n\n**æ¡ˆä¾‹åˆ†æ**ï¼š\n\n- å“ªäº›æ ·æœ¬é¢„æµ‹é”™è¯¯ï¼Ÿ\n- æ˜¯å¦æŸç±»æ ·æœ¬ï¼ˆå¦‚å°å¸‚å€¼ï¼‰æ›´éš¾é¢„æµ‹ï¼Ÿ\n- å¤šæ¨¡æ€åœ¨å“ªäº›æƒ…å†µä¸‹å¸®åŠ©æœ€å¤§ï¼Ÿ\n\n### åˆ‡ç‰‡è¯„ä¼°ä¸ç¨³å¥æ€§\n\n**æ—¶é—´åˆ‡ç‰‡**ï¼š\n\n- è®­ç»ƒæœŸ vs æµ‹è¯•æœŸæ€§èƒ½å·®å¼‚\n- ä¸åŒå¸‚åœºçŠ¶æ€ï¼ˆç‰›å¸‚/ç†Šå¸‚ï¼‰\n\n**æ¨ªæˆªé¢åˆ‡ç‰‡**ï¼š\n\n- ä¸åŒè¡Œä¸šï¼ˆç§‘æŠ€ vs åˆ¶é€ ï¼‰\n- ä¸åŒè§„æ¨¡ï¼ˆå¤§å¸‚å€¼ vs å°å¸‚å€¼ï¼‰\n\n**ç›®çš„**ï¼šç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚\n\n## æœ¬å‘¨å°ç»“\n\n### æ ¸å¿ƒè¦ç‚¹\n\n1. **å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡**ï¼šæ˜ç¡®æ ‡ç­¾å®šä¹‰ï¼ˆä½•æ—¶ã€å¯¹è°ã€é¢„æµ‹ä»€ä¹ˆï¼‰\n2. **å›¾åƒç‰¹å¾**ï¼šCNN/ViT/CLIPï¼Œæ³¨æ„ç©ºé—´å¯¹é½ã€è´¨é‡æ§åˆ¶ã€åŸŸæ¼‚ç§»\n3. **éŸ³é¢‘ç‰¹å¾**ï¼šASR â†’ æ–‡æœ¬ vs å£°å­¦/éŸµå¾‹ç‰¹å¾ï¼Œæ³¨æ„ ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦»\n4. **è§†é¢‘ç‰¹å¾**ï¼šå¸§çº§ + æ—¶åºå»ºæ¨¡ï¼Œè¡Œä¸ºçº¿ç´¢ï¼ˆAU/å§¿æ€/æ³¨è§†ï¼‰ï¼Œæ³¨æ„æµ‹é‡è¯¯å·®\n5. **èåˆç­–ç•¥**ï¼šæ—©/ä¸­/åèåˆã€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤„ç†ç¼ºå¤±æ¨¡æ€\n6. **å¢é‡æ£€éªŒ**ï¼šç›¸å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡çš„å¢é‡è´¡çŒ®ï¼Œäº’è¡¥æ€§è¯æ®\n7. **ä¼¦ç†è¾¹ç•Œ**ï¼šç”Ÿç‰©ç‰¹å¾åˆè§„ã€ç®—æ³•åå·®ã€éšç§ä¿æŠ¤\n\n### æœ¬å‘¨æ€è€ƒé¢˜\n\n#### é—®é¢˜ 1\n\nä»¥\"ä¸šç»©ç”µè¯ä¼šéŸ³é¢‘\"æ„é€ ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚è¯†åˆ«ç®¡ç†å±‚ä¸ç¡®å®šæ€§çŠ¶æ€ï¼‰æ—¶ï¼Œæ ‡ç­¾é€šå¸¸å¦‚ä½•å®šä¹‰ï¼Ÿå“ªäº›ç¯èŠ‚æœ€å®¹æ˜“å¼•å…¥æ ·æœ¬é€‰æ‹©åè¯¯ï¼Ÿ\n\n:::{.callout-note collapse=\"true\"}\nğŸ’¡ å‚è€ƒç­”æ¡ˆ\n\n**æ ‡ç­¾å®šä¹‰**ï¼š\n\n1. **åŸºäºæœªæ¥è‚¡ä»·**ï¼š\n   - $y = \\mathbb{I}(r_{t+1 \\to t+k} < \\text{å¸‚åœºä¸­ä½æ•°})$\n   - ä¸ç¡®å®šæ€§é«˜ â†’ æœªæ¥è¡¨ç°å·®\n\n2. **åŸºäºè´¢åŠ¡æ„å¤–**ï¼š\n   - $y = \\mathbb{I}(|\\text{å®é™…EPS} - \\text{é¢„æœŸEPS}| > \\text{é˜ˆå€¼})$\n   - ä¸ç¡®å®šæ€§é«˜ â†’ é¢„æµ‹è¯¯å·®å¤§\n\n3. **åŸºäºæ–‡æœ¬è¯å…¸**ï¼ˆè¾…åŠ©æ ‡æ³¨ï¼‰ï¼š\n   - ç»Ÿè®¡ä¸ç¡®å®šæ€§è¯æ±‡ï¼ˆ\"uncertain\", \"might\", \"could\"ï¼‰\n   - äººå·¥éªŒè¯éƒ¨åˆ†æ ·æœ¬\n\n**æ ·æœ¬é€‰æ‹©åè¯¯æ¥æº**ï¼š\n\n1. **å¯å¾—æ€§åè¯¯**\n   - å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€å½•éŸ³\n   - å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å¯èƒ½éšè—\n\n2. **å­˜æ´»åè¯¯**\n   - é€€å¸‚å…¬å¸çš„ç”µè¯ä¼šæ•°æ®ç¼ºå¤±\n\n3. **æ—¶æœŸé€‰æ‹©**\n   - å±æœºæœŸé—´ç”µè¯ä¼šå–æ¶ˆç‡æ›´é«˜\n\n**ç¼“è§£**ï¼š\n\n- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾åˆ†å¸ƒ\n- å€¾å‘è¯„åˆ†åŠ æƒ\n- ç¨³å¥æ€§æ£€éªŒï¼ˆä»…ç”¨æŒç»­æœ‰æ•°æ®çš„å…¬å¸ï¼‰\n:::\n\n#### é—®é¢˜ 2\n\nå¦‚ä½•ä¸¥è°¨åœ°è¯æ˜å¤šæ¨¡æ€ä¿¡å·å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡å…·æœ‰å¢é‡ä¿¡æ¯ï¼Ÿè¯·ç»™å‡ºä¸€ä¸ªä½ è®¤ä¸ºå¿…è¦çš„äº’è¡¥æ€§æ£€éªŒè®¾è®¡ã€‚\n\n:::{.callout-note collapse=\"true\"}\nğŸ’¡ å‚è€ƒç­”æ¡ˆ\n\n**å®Œæ•´æ£€éªŒæµç¨‹**ï¼š\n\n**1. åŸºå‡†å¯¹æ¯”**\n\n| æ¨¡å‹ | ç‰¹å¾ | AUC | $\\Delta$ AUC |\n|-----|------|-----|-------------|\n| M1 | ç»“æ„åŒ–ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰ | 0.72 | - |\n| M2 | M1 + æ–‡æœ¬ | 0.76 | +0.04 |\n| M3 | M2 + éŸ³é¢‘ | 0.79 | +0.03 |\n\n- **æ¡ä»¶**ï¼š$\\Delta$ AUC ç»Ÿè®¡æ˜¾è‘—ï¼ˆDeLong æ£€éªŒ $p < 0.05$ï¼‰\n\n**2. ç›¸å…³æ€§åˆ†æ**\n\n```python\ncorr_matrix = df[['structured', 'text', 'audio']].corr()\nassert corr_matrix.loc['audio', 'text'] < 0.6  # é¿å…é«˜åº¦å†—ä½™\n```\n\n**3. æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ**\n\nåœ¨æ§åˆ¶æ–‡æœ¬åï¼ŒéŸ³é¢‘æ˜¯å¦ä»æœ‰é¢„æµ‹åŠ›ï¼Ÿ\n\n$$\n\\text{Logit}(y) = \\beta_0 + \\beta_1 \\text{Text} + \\beta_2 \\text{Audio} + \\varepsilon\n$$\n\n- æ£€éªŒï¼š$\\beta_2$ æ˜¯å¦æ˜¾è‘—ï¼Ÿ\n\n**4. åˆ†ç»„å¼‚è´¨æ€§**\n\nåœ¨æ–‡æœ¬ä¿¡å·å¼±çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘è´¡çŒ®æ›´å¤§ï¼š\n\n```\nåˆ†ç»„ï¼šæ–‡æœ¬æƒ…æ„Ÿ = ä¸­æ€§ï¼ˆä¿¡æ¯é‡å°‘ï¼‰\n  â†’ éŸ³é¢‘ AUC æå‡æ›´æ˜¾è‘—\n```\n\n**5. æ¶ˆèå®éªŒ**\n\n| ç§»é™¤æ¨¡æ€ | AUC ä¸‹é™ | è§£é‡Š |\n|---------|---------|-----|\n| å»é™¤éŸ³é¢‘ | -0.03 | éŸ³é¢‘æœ‰ç‹¬ç‰¹è´¡çŒ® |\n| å»é™¤æ–‡æœ¬ | -0.04 | æ–‡æœ¬ä»é‡è¦ |\n| å»é™¤ç»“æ„åŒ– | -0.05 | åŸºç¡€å˜é‡æœ€é‡è¦ |\n\n**ç»“è®ºæ¨¡æ¿**ï¼š\n\n\"éŸ³é¢‘ç‰¹å¾åœ¨æ§åˆ¶æ–‡æœ¬ä¸ç»“æ„åŒ–å˜é‡åï¼Œä»èƒ½å¸¦æ¥ç»Ÿè®¡æ˜¾è‘—çš„ AUC æå‡ï¼ˆ+0.03, p=0.01ï¼‰ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºéŸ³é¢‘ä¸æ–‡æœ¬ç›¸å…³ç³»æ•°ä¸º 0.45ï¼Œè¡¨æ˜äºŒè€…æ•æ‰äº†éƒ¨åˆ†é‡å ä½†éå®Œå…¨å†—ä½™çš„ä¿¡æ¯ã€‚åœ¨æ–‡æœ¬æƒ…æ„Ÿä¸­æ€§çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘çš„å¢é‡è´¡çŒ®æ›´å¤§ï¼ˆAUC +0.05ï¼‰ï¼Œè¯å®äº†äº’è¡¥æ€§ã€‚\"\n:::\n\n---\n\n### è¯¾ç¨‹å‰åŠæ®µå›é¡¾\n\nå‰å››å‘¨æˆ‘ä»¬å»ºç«‹äº†**AI èµ‹èƒ½é‡‘èç ”ç©¶çš„å®Œæ•´æ¡†æ¶**ï¼š\n\n- **ç¬¬1å‘¨**ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆi.i.d. åœºæ™¯ï¼‰\n- **ç¬¬2å‘¨**ï¼šé‡‘èæ—¶åºè¯„ä¼°ä¸å›æµ‹è§„èŒƒ\n- **ç¬¬3å‘¨**ï¼šæ–‡æœ¬åˆ†æçš„å¯å®¡è®¡æµç¨‹\n- **ç¬¬4å‘¨**ï¼šå¤šæ¨¡æ€ä¿¡å·çš„å¢é‡æ£€éªŒ\n\n**å…±åŒä¸»çº¿**ï¼š\n\n1. **æ³›åŒ–èƒ½åŠ›**ï¼šæ ·æœ¬å¤–è¡¨ç°æ˜¯å”¯ä¸€æ ‡å‡†\n2. **å¯å®¡è®¡æ€§**ï¼šä»åŸå§‹æ•°æ®åˆ°å˜é‡çš„æ¯ä¸€æ­¥å¯è¿½æº¯\n3. **å¢é‡ä»·å€¼**ï¼šæ–°æ–¹æ³•å¿…é¡»ç›¸å¯¹å·²çŸ¥æ–¹æ³•å±•ç¤ºå¢é‡ä¿¡æ¯\n\n### ååŠæ®µè¯¾ç¨‹é¢„å‘Š\n\n**ç¬¬5-8å‘¨**ï¼šå­¦ç”Ÿç ”ç©¶è®¡åˆ’æ±‡æŠ¥ä¸ç ”è®¨\n\n- æ¯ç»„å±•ç¤ºç ”ç©¶é—®é¢˜ã€æ•°æ®ã€æ–¹æ³•ã€è¯„ä¼°åè®®\n- è¯¾å ‚è®¨è®ºä¸åé¦ˆ\n- å¼ºåŒ–**æœ€å°è¯æ®é“¾**æ„è¯†\n\n**ç¬¬5å‘¨å‰**å°†æœ‰**è¯¾å‰æµ‹éªŒ**ï¼Œè¦†ç›–ç¬¬1-4å‘¨æ ¸å¿ƒæ¦‚å¿µï¼Œè¯·æå‰å¤ä¹ ï¼\n\n---\n\n**ç¥å­¦ä¹ é¡ºåˆ©ï¼æœŸå¾…çœ‹åˆ°ä½ ä»¬çš„ç²¾å½©ç ”ç©¶ï¼**\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"html-math-method":"katex","output-file":"week4.html"},"language":{"toc-title-document":"ç›®å½•","toc-title-website":"è¯¥é¡µé¢å†…å®¹","related-formats-title":"å…¶ä»–æ ¼å¼","related-notebooks-title":"ç¬”è®°æœ¬","source-notebooks-prefix":"èµ„æº","other-links-title":"å…¶ä»–é“¾æ¥","code-links-title":"ä»£ç é“¾æ¥","launch-dev-container-title":"å¯åŠ¨ Dev Container","launch-binder-title":"å¯åŠ¨ Binder","article-notebook-label":"æ–‡ç« ç¬”è®°æœ¬","notebook-preview-download":"ä¸‹è½½ç¬”è®°æœ¬","notebook-preview-download-src":"ä¸‹è½½æºä»£ç ","notebook-preview-back":"è¿”å›æ–‡ç« ","manuscript-meca-bundle":"MECA å­˜æ¡£","section-title-abstract":"æ‘˜è¦","section-title-appendices":"é™„å½•","section-title-footnotes":"è„šæ³¨","section-title-references":"å‚è€ƒæ–‡çŒ®","section-title-reuse":"äºŒæ¬¡ä½¿ç”¨","section-title-copyright":"ç‰ˆæƒ","section-title-citation":"å¼•ç”¨æ ¼å¼","appendix-attribution-cite-as":"è¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"æŸ¥çœ‹è®¸å¯åè®®","title-block-author-single":"ä½œè€…","title-block-author-plural":"ä½œè€…","title-block-affiliation-single":"å•ä½","title-block-affiliation-plural":"å•ä½","title-block-published":"å‘å¸ƒäº","title-block-modified":"ä¿®æ”¹äº","title-block-keywords":"å…³é”®è¯","callout-tip-title":"æç¤º","callout-note-title":"æ³¨è®°","callout-warning-title":"è­¦å‘Š","callout-important-title":"é‡è¦","callout-caution-title":"æ³¨æ„","code-summary":"ä»£ç ","code-tools-menu-caption":"ä»£ç ","code-tools-show-all-code":"æ˜¾ç¤ºæ‰€æœ‰ä»£ç ","code-tools-hide-all-code":"éšè—æ‰€æœ‰ä»£ç ","code-tools-view-source":"æŸ¥çœ‹æºä»£ç ","code-tools-source-code":"æºä»£ç ","tools-share":"Share","tools-download":"Download","code-line":"è¡Œ","code-lines":"è¡Œ","copy-button-tooltip":"å¤åˆ¶åˆ°å‰ªè´´æ¿","copy-button-tooltip-success":"å·²å¤åˆ¶","repo-action-links-edit":"ç¼–è¾‘è¯¥é¡µé¢","repo-action-links-source":"æŸ¥çœ‹ä»£ç ","repo-action-links-issue":"åé¦ˆé—®é¢˜","back-to-top":"å›åˆ°é¡¶éƒ¨","search-no-results-text":"æ²¡æœ‰ç»“æœ","search-matching-documents-text":"åŒ¹é…çš„æ–‡æ¡£","search-copy-link-title":"å¤åˆ¶æœç´¢é“¾æ¥","search-hide-matches-text":"éšè—å…¶å®ƒåŒ¹é…ç»“æœ","search-more-match-text":"æ›´å¤šåŒ¹é…ç»“æœ","search-more-matches-text":"æ›´å¤šåŒ¹é…ç»“æœ","search-clear-button-title":"æ¸…é™¤","search-text-placeholder":"","search-detached-cancel-button-title":"å–æ¶ˆ","search-submit-button-title":"æäº¤","search-label":"æœç´¢","toggle-section":"å±•å¼€æˆ–æŠ˜å æ­¤æ ","toggle-sidebar":"å±•å¼€æˆ–æŠ˜å ä¾§è¾¹æ å¯¼èˆª","toggle-dark-mode":"åˆ‡æ¢æ·±è‰²æ¨¡å¼","toggle-reader-mode":"åˆ‡æ¢é˜…è¯»å™¨æ¨¡å¼","toggle-navigation":"å±•å¼€æˆ–æŠ˜å å¯¼èˆªæ ","crossref-fig-title":"å›¾","crossref-tbl-title":"è¡¨","crossref-lst-title":"åˆ—è¡¨","crossref-thm-title":"å®šç†","crossref-lem-title":"å¼•ç†","crossref-cor-title":"æ¨è®º","crossref-prp-title":"å‘½é¢˜","crossref-cnj-title":"çŒœæƒ³","crossref-def-title":"å®šä¹‰","crossref-exm-title":"ä¾‹","crossref-exr-title":"ä¹ é¢˜","crossref-ch-prefix":"ç« èŠ‚","crossref-apx-prefix":"é™„å½•","crossref-sec-prefix":"å°èŠ‚","crossref-eq-prefix":"å¼","crossref-lof-title":"å›¾ç´¢å¼•","crossref-lot-title":"è¡¨ç´¢å¼•","crossref-lol-title":"åˆ—è¡¨ç´¢å¼•","environment-proof-title":"è¯","environment-remark-title":"æ³¨è®°","environment-solution-title":"è§£","listing-page-order-by":"æ’åºæ–¹å¼","listing-page-order-by-default":"é»˜è®¤","listing-page-order-by-date-asc":"æ—¥æœŸå‡åº","listing-page-order-by-date-desc":"æ—¥æœŸé™åº","listing-page-order-by-number-desc":"é™åº","listing-page-order-by-number-asc":"å‡åº","listing-page-field-date":"æ—¥æœŸ","listing-page-field-title":"æ ‡é¢˜","listing-page-field-description":"æè¿°","listing-page-field-author":"ä½œè€…","listing-page-field-filename":"æ–‡ä»¶å","listing-page-field-filemodified":"ä¿®æ”¹æ—¶é—´","listing-page-field-subtitle":"å‰¯æ ‡é¢˜","listing-page-field-readingtime":"é˜…è¯»æ—¶é—´","listing-page-field-wordcount":"å­—æ•°ç»Ÿè®¡","listing-page-field-categories":"åˆ†ç±»","listing-page-minutes-compact":"{0} åˆ†é’Ÿ","listing-page-category-all":"å…¨éƒ¨","listing-page-no-matches":"æ— åŒ¹é…é¡¹","listing-page-words":"{0} å­—","listing-page-filter":"ç­›é€‰","draft":"è‰ç¨¿"},"metadata":{"lang":"zh","fig-responsive":true,"quarto-version":"1.6.32","theme":"cosmo","title":"ç¬¬4è®²ï¼šå¤šæ¨¡æ€æ¨¡å‹ç†è®º â†’ é‡‘èä¿¡å·æŠ½å–","subtitle":"å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘çš„ç‰¹å¾æå–ä¸å¢é‡ä¿¡æ¯æ£€éªŒ"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}