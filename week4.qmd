---
title: "ç¬¬4è®²ï¼šå¤šæ¨¡æ€æ¨¡å‹ç†è®º â†’ é‡‘èä¿¡å·æŠ½å–"
subtitle: "å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘çš„ç‰¹å¾æå–ä¸å¢é‡ä¿¡æ¯æ£€éªŒ"
---

# ä»»åŠ¡ä¸æ•°æ®ï¼šæŠŠå¤šæ¨¡æ€ç ”ç©¶è¡¨è¿°ä¸ºåˆ†ç±»é—®é¢˜

## ä¸ºä»€ä¹ˆæ˜¯"åˆ†ç±»"ï¼Ÿ

è™½ç„¶å¤šæ¨¡æ€æ•°æ®å½¢å¼å¤šæ ·ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ï¼Œä½†åœ¨é‡‘èåº”ç”¨ä¸­ï¼Œæ ¸å¿ƒä»»åŠ¡é€šå¸¸å¯è¡¨è¿°ä¸º**åˆ†ç±»é—®é¢˜**ï¼š

- é¢„æµ‹**æ¶¨/è·Œ**ï¼ˆäºŒåˆ†ç±»ï¼‰
- è¯†åˆ«**ç®¡ç†å±‚æƒ…ç»ªçŠ¶æ€**ï¼ˆå¤šåˆ†ç±»ï¼šä¹è§‚/ä¸­æ€§/æ‚²è§‚ï¼‰
- åˆ¤æ–­**æ¬ºè¯ˆ/éæ¬ºè¯ˆ**ï¼ˆäºŒåˆ†ç±»ï¼‰
- é¢„æµ‹**ä¿¡ç”¨ç­‰çº§**ï¼ˆå¤šåˆ†ç±»ï¼‰

:::{.callout-note icon=false}
### ğŸ¯ æ ‡ç­¾å®šä¹‰çš„ä¸‰è¦ç´ 

1. **ä½•æ—¶**ï¼ˆWhenï¼‰ï¼šé¢„æµ‹å“ªä¸ªæ—¶æœŸçš„ç»“æœï¼Ÿ
2. **å¯¹è°**ï¼ˆWhoï¼‰ï¼šä¸ªè‚¡/è¡Œä¸š/å¸‚åœºï¼Ÿ
3. **é¢„æµ‹ä»€ä¹ˆ**ï¼ˆWhatï¼‰ï¼šå…·ä½“çš„ç±»åˆ«å®šä¹‰ï¼Ÿ

**ç¤ºä¾‹**ï¼š

- **ä½•æ—¶**ï¼šä¸šç»©ç”µè¯ä¼šåç¬¬äºŒä¸ªäº¤æ˜“æ—¥
- **å¯¹è°**ï¼šè¯¥å…¬å¸è‚¡ç¥¨
- **é¢„æµ‹ä»€ä¹ˆ**ï¼šæ”¶ç›Šç‡æ˜¯å¦è¶…è¿‡å¸‚åœºä¸­ä½æ•°ï¼ˆäºŒåˆ†ç±»ï¼‰

æ ‡ç­¾å®šä¹‰å†³å®šäº†ï¼š

- æ•°æ®å¯¹é½æ–¹å¼ï¼ˆä½•æ—¶æå–ç‰¹å¾ï¼‰
- è¯„ä¼°åè®®ï¼ˆæ—¶åº CV çš„çª—å£è®¾ç½®ï¼‰
- æ ·æœ¬é€‰æ‹©ï¼ˆæ˜¯å¦åŒ…å«åœç‰Œè‚¡ç¥¨ï¼‰
:::

## å¤šæ¨¡æ€æ•°æ®ç‰ˆå›¾

### å›¾åƒï¼ˆImageï¼‰

| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |
|-------|---------|-----|
| **å«æ˜Ÿå›¾åƒ** | é¢„æµ‹é›¶å”®å®¢æµã€å·¥å‚æ´»åŠ¨ã€å•†å“åº“å­˜ | ç©ºé—´å¯¹é½ã€äº‘å±‚é®æŒ¡ |
| **å¤œå…‰æ•°æ®** | åŒºåŸŸç»æµæ´»åŠ¨å¼ºåº¦ | åˆ†è¾¨ç‡ä½ã€å­£èŠ‚æ€§ |
| **ç¥¨æ®/åˆåŒæ‰«æä»¶** | OCR æå–ä¿¡æ¯ã€æ¬ºè¯ˆæ£€æµ‹ | ç‰ˆå¼å¤šæ ·ã€å™ªå£°å¤§ |
| **è´¢æŠ¥å›¾è¡¨æˆªå›¾** | è‡ªåŠ¨æå–æ•°æ®è¶‹åŠ¿ | ä¿¡æ¯æ³„éœ²é£é™©ï¼ˆéœ€ç¡®è®¤å‘å¸ƒæ—¶é—´ï¼‰|
| **ç¤¾äº¤åª’ä½“å›¾ç‰‡** | å“ç‰Œå½¢è±¡ç›‘æµ‹ã€äº§å“è¯†åˆ« | æ ·æœ¬é€‰æ‹©åè¯¯ |

### éŸ³é¢‘ï¼ˆAudioï¼‰

| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |
|-------|---------|-----|
| **è´¢æŠ¥ç”µè¯ä¼š** | ç®¡ç†å±‚æƒ…ç»ªã€ä¸ç¡®å®šæ€§è¯†åˆ« | ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦» |
| **å®¢æœå½•éŸ³** | å®¢æˆ·æ»¡æ„åº¦ã€æŠ•è¯‰é¢„è­¦ | éšç§åˆè§„ã€ä¿¡é“å™ªå£° |
| **äº¤æ˜“å‘˜é€šè¯** | åˆè§„ç›‘æ§ã€å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ | å®æ—¶æ€§è¦æ±‚ã€æœ¯è¯­è¯†åˆ« |

### è§†é¢‘ï¼ˆVideoï¼‰

| æ•°æ®æº | é‡‘èåº”ç”¨ | æŒ‘æˆ˜ |
|-------|---------|-----|
| **ç®¡ç†å±‚è·¯æ¼”/å‘å¸ƒä¼š** | éè¨€è¯­çº¿ç´¢ï¼ˆé¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿ï¼‰| è§’åº¦/å…‰ç…§å˜åŒ–ã€é®æŒ¡ |
| **ç›‘æ§è§†é¢‘** | ç½‘ç‚¹å®¢æµã€ATM å¼‚å¸¸ | è®¡ç®—æˆæœ¬é«˜ã€éšç§é—®é¢˜ |

# è§†è§‰ï¼ˆå›¾åƒï¼‰æ¨¡æ€ï¼šç‰¹å¾æå–ä¸å˜é‡æ„é€ 

## å›¾åƒè¡¨ç¤ºæ–¹æ³•æ¼”è¿›

### ä¼ ç»Ÿæ–¹æ³•ï¼šæ‰‹å·¥ç‰¹å¾

**ä½å±‚ç‰¹å¾**ï¼š

- é¢œè‰²ç›´æ–¹å›¾
- çº¹ç†ï¼ˆGabor æ»¤æ³¢å™¨ã€LBPï¼‰
- è¾¹ç¼˜æ£€æµ‹ï¼ˆCannyã€Sobelï¼‰

**ä¸­å±‚ç‰¹å¾**ï¼š

- SIFTï¼ˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢ï¼‰
- HOGï¼ˆæ–¹å‘æ¢¯åº¦ç›´æ–¹å›¾ï¼‰

**é—®é¢˜**ï¼š

- ç‰¹å¾è®¾è®¡ä¾èµ–ä¸“å®¶çŸ¥è¯†
- æ³›åŒ–èƒ½åŠ›æœ‰é™

### æ·±åº¦å­¦ä¹ ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

**ç»å…¸æ¶æ„æ¼”è¿›**ï¼š

```
AlexNet (2012) â†’ VGGNet (2014) â†’ ResNet (2015) â†’ EfficientNet (2019)
```

**æ ¸å¿ƒæ€æƒ³**ï¼š

é€šè¿‡å¤šå±‚å·ç§¯+æ± åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ ä»ä½å±‚ï¼ˆè¾¹ç¼˜ï¼‰åˆ°é«˜å±‚ï¼ˆç‰©ä½“ï¼‰çš„ç‰¹å¾å±‚æ¬¡ã€‚

**ç¤ºä¾‹ï¼šResNet-50**

```python
from torchvision.models import resnet50, ResNet50_Weights

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
model.eval()

# æå–ç‰¹å¾ï¼ˆå»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼‰
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

# è¾“å…¥å›¾åƒ â†’ 2048ç»´ç‰¹å¾å‘é‡
image_tensor = preprocess(image)  # (3, 224, 224)
features = feature_extractor(image_tensor.unsqueeze(0))  # (1, 2048, 1, 1)
features = features.squeeze()  # (2048,)
```

### æœ€æ–°ï¼šè§†è§‰Transformerï¼ˆViTï¼‰

**æ€æƒ³**ï¼š

å°†å›¾åƒåˆ‡åˆ†ä¸º patchesï¼Œç”¨ Transformer å¤„ç†ï¼ˆç±»ä¼¼ NLP ä¸­çš„è¯ï¼‰ã€‚

**ç¤ºä¾‹**ï¼š

```python
from transformers import ViTModel, ViTImageProcessor

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTModel.from_pretrained('google/vit-base-patch16-224')

inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
image_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token
```

### è‡ªç›‘ç£å­¦ä¹ ï¼šCLIPï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼ˆRadford et al., 2021ï¼‰ï¼š

è”åˆè®­ç»ƒå›¾åƒç¼–ç å™¨ä¸æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹åœ¨åµŒå…¥ç©ºé—´ä¸­æ¥è¿‘ã€‚

**ä¼˜åŠ¿**ï¼š

- **é›¶æ ·æœ¬è¿ç§»**ï¼šæ— éœ€æ ‡æ³¨æ•°æ®å³å¯åˆ†ç±»
- **è·¨æ¨¡æ€æ£€ç´¢**ï¼šç”¨æ–‡æœ¬æŸ¥è¯¢å›¾åƒï¼Œæˆ–åä¹‹

**ç¤ºä¾‹**ï¼š

```python
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å›¾åƒ + æ–‡æœ¬
image = load_image("store.jpg")
texts = ["a busy retail store", "an empty store"]

inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)

# è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦
logits_per_image = outputs.logits_per_image  # (1, 2)
probs = logits_per_image.softmax(dim=1)  # [0.8, 0.2] â†’ "busy"
```

## é‡‘èåº”ç”¨æ¡ˆä¾‹

### 1. å«æ˜Ÿå›¾åƒé¢„æµ‹ç»æµæ´»åŠ¨

**ä»»åŠ¡**ï¼šç”¨åœè½¦åœºè½¦è¾†æ•°é‡é¢„æµ‹é›¶å”®å•†å®¢æµã€‚

**æµç¨‹**ï¼š

```
å«æ˜Ÿå›¾åƒï¼ˆå•†åœºåœè½¦åœºï¼‰
    â†“
ç›®æ ‡æ£€æµ‹ï¼ˆYOLO/Faster R-CNNï¼‰â†’ è½¦è¾†æ•°é‡
    â†“
æ—¶é—´åºåˆ—èšåˆ â†’ å‘¨/æœˆå¹³å‡è½¦è¾†æ•°
    â†“
å¯¹é½åˆ°å…¬å¸ â†’ é¢„æµ‹é”€å”®é¢/è‚¡ä»·
```

**ç»å…¸æ–‡çŒ®**ï¼š

- Naik et al. (2019): *Measuring Economic Activity from Space*

**å…³é”®æŒ‘æˆ˜**ï¼š

- **ç©ºé—´å¯¹é½**ï¼šåœè½¦åœº â†’ å…·ä½“é—¨åº— â†’ ä¸Šå¸‚å…¬å¸
- **äº‘å±‚é®æŒ¡**ï¼šç¼ºå¤±æ•°æ®å¤„ç†
- **å­£èŠ‚æ€§**ï¼šèŠ‚å‡æ—¥/å¤©æ°”å½±å“

### 2. è´¢æŠ¥å›¾è¡¨è‡ªåŠ¨æå–

**ä»»åŠ¡**ï¼šä»å¹´æŠ¥ PDF ä¸­æå–è¶‹åŠ¿å›¾ï¼Œè¿˜åŸæ•°æ®ã€‚

**æµç¨‹**ï¼š

```
PDF â†’ å›¾åƒæå– â†’ å›¾è¡¨æ£€æµ‹ â†’ OCR æ•°å­— â†’ æ•°æ®é‡å»º
```

**å·¥å…·**ï¼š

- **å›¾è¡¨æ£€æµ‹**ï¼šDetectron2
- **OCR**ï¼šTesseract, PaddleOCR
- **æ•°æ®æå–**ï¼šChartOCR (ä¸“ç”¨å·¥å…·)

**é£é™©**ï¼š

- **ä¿¡æ¯æ³„éœ²**ï¼šå¿…é¡»ç¡®è®¤å›¾è¡¨åœ¨é¢„æµ‹æ—¶ç‚¹å·²å‘å¸ƒ
- **æµ‹é‡è¯¯å·®**ï¼šOCR è¯†åˆ«é”™è¯¯

### 3. ç¤¾äº¤åª’ä½“å›¾ç‰‡åˆ†æ

**ä»»åŠ¡**ï¼šå“ç‰Œæ›å…‰åº¦ç›‘æµ‹ã€‚

**æµç¨‹**ï¼š

```
Instagram/Twitter å›¾ç‰‡
    â†“
å“ç‰Œ Logo æ£€æµ‹ï¼ˆYOLO å¾®è°ƒï¼‰
    â†“
èšåˆï¼šæ¯æ—¥å“ç‰Œæ›å…‰æ¬¡æ•°
    â†“
é¢„æµ‹ï¼šå“ç‰Œä»·å€¼ã€è‚¡ä»·
```

**æŒ‘æˆ˜**ï¼š

- **æ ·æœ¬é€‰æ‹©åè¯¯**ï¼šç¤¾äº¤åª’ä½“ç”¨æˆ·ä¸ä»£è¡¨æ•´ä½“äººç¾¤
- **æ—¶é—´å¯¹é½**ï¼šå›¾ç‰‡å‘å¸ƒæ—¶é—´ vs å¸‚åœºååº”

## å…³é”®å¯¹é½ä¸åè¯¯

### 1. æ‹æ‘„æ—¥ â‰  æŠ«éœ²æ—¥

**é—®é¢˜**ï¼š

- å«æ˜Ÿå›¾åƒæ‹æ‘„äº $t$ æ—¥ï¼Œä½†ä¸‹è½½/å¤„ç†åæ‰åœ¨ $t+k$ æ—¥å¯ç”¨
- è‹¥ç”¨ $t$ æ—¥å›¾åƒé¢„æµ‹ $t$ æ—¥æ”¶ç›Š â†’ **å‰ç»åè¯¯**

**è§£å†³**ï¼š

```python
# è®°å½•å›¾åƒæ‹æ‘„æ—¶é—´ä¸è·å–æ—¶é—´
df['image_date'] = '2024-01-15'     # å«æ˜Ÿè¿‡å¢ƒæ—¶é—´
df['available_date'] = '2024-01-18' # å›¾åƒä¸‹è½½/å¤„ç†å®Œæˆæ—¶é—´

# ç”¨ available_date å¯¹é½åˆ°äº¤æ˜“æ—¥
df['trade_date'] = df['available_date'].apply(next_trading_day)
```

### 2. ç©ºé—´å¯¹é½

**é—®é¢˜**ï¼š

- å«æ˜Ÿå›¾åƒè¦†ç›–åŒºåŸŸ â†’ å¤šä¸ªé—¨åº— â†’ å¤šä¸ªå…¬å¸

**è§£å†³**ï¼š

- ä½¿ç”¨**åœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰**ç²¾ç¡®åŒ¹é…
- è®°å½•åŒ¹é…è§„åˆ™ä¸è¯¯å·®èŒƒå›´

### 3. åŸŸæ¼‚ç§»ï¼ˆDomain Shiftï¼‰

**é—®é¢˜**ï¼š

- æ¨¡å‹åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒï¼ˆè‡ªç„¶å›¾åƒï¼‰
- åº”ç”¨äºé‡‘èå›¾åƒï¼ˆå«æ˜Ÿã€ç¥¨æ®ï¼‰æ—¶æ€§èƒ½ä¸‹é™

**è§£å†³**ï¼š

- **å¾®è°ƒ**ï¼ˆFine-tuningï¼‰ï¼šåœ¨ç›®æ ‡åŸŸä¸Šç»§ç»­è®­ç»ƒ
- **é¢†åŸŸè‡ªé€‚åº”**ï¼šå‡å°‘æºåŸŸä¸ç›®æ ‡åŸŸçš„åˆ†å¸ƒå·®å¼‚

### 4. è´¨é‡æ§åˆ¶

**å¸¸è§é—®é¢˜**ï¼š

- äº‘å±‚é®æŒ¡ï¼ˆå«æ˜Ÿå›¾åƒï¼‰
- å¤œé—´å›¾åƒè¿‡æš—
- å‹ç¼©å¤±çœŸ

**è§£å†³**ï¼š

- å›¾åƒè´¨é‡è¯„åˆ†ï¼ˆæ¨¡ç³Šåº¦ã€äº®åº¦æ£€æµ‹ï¼‰
- å‰”é™¤ä½è´¨é‡æ ·æœ¬

# éŸ³é¢‘æ¨¡æ€ï¼šç‰¹å¾æå–ä¸æµ‹é‡è¯¯å·®

## ä¸¤æ¡ç‰¹å¾æå–ç®¡çº¿

### ç®¡çº¿ 1ï¼šASR â†’ æ–‡æœ¬ï¼ˆæ¥ç¬¬3å‘¨æµç¨‹ï¼‰

**Automatic Speech Recognitionï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰**

**æµç¨‹**ï¼š

```
éŸ³é¢‘æ–‡ä»¶ï¼ˆ.wav/.mp3ï¼‰
    â†“
ASR æ¨¡å‹ï¼ˆWhisper/Google Cloud Speechï¼‰
    â†“
æ–‡æœ¬è½¬å½•
    â†“
åº”ç”¨ç¬¬3å‘¨çš„æ–‡æœ¬åˆ†ææ–¹æ³•
```

**å·¥å…·**ï¼š

```python
import whisper

# OpenAI Whisperï¼ˆå¼€æºï¼Œé«˜ç²¾åº¦ï¼‰
model = whisper.load_model("base")
result = model.transcribe("earnings_call.mp3")
text = result["text"]
```

**ä¼˜ç‚¹**ï¼š

- å¯ç”¨æˆç†Ÿçš„ NLP æ–¹æ³•
- å¯è§£é‡Šæ€§å¼º

**ç¼ºç‚¹**ï¼š

- **ASR é”™è¯¯**ï¼šè¯†åˆ«ä¸å‡†ï¼ˆç‰¹åˆ«æ˜¯ä¸“ä¸šæœ¯è¯­ã€å£éŸ³ï¼‰
- **ä¸¢å¤±éŸµå¾‹ä¿¡æ¯**ï¼šè¯­è°ƒã€åœé¡¿ã€è¯­é€Ÿ

### ç®¡çº¿ 2ï¼šç›´æ¥æå–å£°å­¦/éŸµå¾‹ç‰¹å¾

**ä¸è½¬æ–‡æœ¬ï¼Œç›´æ¥ä»éŸ³é¢‘æå–ç‰¹å¾**ï¼š

| ç‰¹å¾ç±»åˆ« | å…·ä½“ç‰¹å¾ | é‡‘èå«ä¹‰ |
|---------|---------|---------|
| **éŸµå¾‹ï¼ˆProsodyï¼‰** | éŸ³é«˜ï¼ˆpitchï¼‰ã€è¯­é€Ÿï¼ˆspeaking rateï¼‰ã€åœé¡¿ï¼ˆpauseï¼‰ | æƒ…ç»ªã€ç´§å¼ åº¦ |
| **éŸ³è´¨ï¼ˆVoice qualityï¼‰** | é¢¤éŸ³ï¼ˆjitterï¼‰ã€æµŠéŸ³ï¼ˆshimmerï¼‰ | å‹åŠ›ã€ä¸ç¡®å®šæ€§ |
| **èƒ½é‡** | éŸ³é‡ã€èƒ½é‡åˆ†å¸ƒ | å¼ºè°ƒã€ä¿¡å¿ƒ |
| **ä½å±‚ç‰¹å¾** | MFCCï¼ˆæ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°ï¼‰ | é€šç”¨éŸ³é¢‘è¡¨ç¤º |
| **æ·±åº¦åµŒå…¥** | Wav2Vec 2.0, HuBERT | ç«¯åˆ°ç«¯å­¦ä¹  |

### æå–ç¤ºä¾‹ï¼šéŸµå¾‹ç‰¹å¾

```python
import librosa
import numpy as np

# åŠ è½½éŸ³é¢‘
y, sr = librosa.load("audio.wav", sr=16000)

# 1. éŸ³é«˜ï¼ˆåŸºé¢‘ï¼‰
pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
pitch_mean = np.mean(pitches[pitches > 0])

# 2. è¯­é€Ÿï¼ˆé€šè¿‡é›¶äº¤å‰ç‡ç²—ç•¥ä¼°è®¡ï¼‰
zcr = librosa.feature.zero_crossing_rate(y)
speaking_rate = np.mean(zcr)

# 3. åœé¡¿ï¼ˆæ£€æµ‹é™éŸ³æ®µï¼‰
intervals = librosa.effects.split(y, top_db=20)
pause_count = len(intervals) - 1

features = {
    'pitch_mean': pitch_mean,
    'speaking_rate': speaking_rate,
    'pause_count': pause_count,
}
```

### æå–ç¤ºä¾‹ï¼šæ·±åº¦åµŒå…¥ï¼ˆWav2Vec 2.0ï¼‰

```python
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torch

processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")

# éŸ³é¢‘ â†’ ç‰¹å¾å‘é‡
input_values = processor(y, sampling_rate=sr, return_tensors="pt").input_values
with torch.no_grad():
    hidden_states = model(input_values).last_hidden_state

# æ—¶é—´å¹³å‡æ± åŒ– â†’ å›ºå®šé•¿åº¦å‘é‡
audio_embedding = hidden_states.mean(dim=1)  # (1, 768)
```

## é‡‘èåº”ç”¨æ¡ˆä¾‹

### 1. è´¢æŠ¥ç”µè¯ä¼šæƒ…ç»ªè¯†åˆ«

**ä»»åŠ¡**ï¼šè¯†åˆ«ç®¡ç†å±‚çš„ä¸ç¡®å®šæ€§/ç´§å¼ åº¦ã€‚

**ç‰¹å¾**ï¼š

- **æ–‡æœ¬**ï¼šè¯å…¸æ³•ï¼ˆä¸ç¡®å®šæ€§è¯æ±‡ï¼‰
- **éŸµå¾‹**ï¼šéŸ³é«˜æ–¹å·®ã€è¯­é€Ÿã€åœé¡¿é¢‘ç‡
- **ç»“åˆ**ï¼šå¤šæ¨¡æ€èåˆ

**ç»å…¸æ–‡çŒ®**ï¼š

- Mayew & Venkatachalam (2012): *The Power of Voice*
- Larcker & Zakolyukina (2012): *Detecting Deceptive Discussions*

**å‘ç°**ï¼š

- éŸ³é«˜å‡é«˜ã€åœé¡¿å¢å¤š â†’ ä¸ç¡®å®šæ€§é«˜
- å¯¹æœªæ¥è‚¡ä»·æœ‰é¢„æµ‹åŠ›ï¼ˆç‹¬ç«‹äºæ–‡æœ¬ï¼‰

### 2. å®¢æœå½•éŸ³è´¨é‡ç›‘æ§

**ä»»åŠ¡**ï¼šè¯†åˆ«å®¢æˆ·ä¸æ»¡æƒ…ç»ªï¼Œé¢„è­¦æŠ•è¯‰ã€‚

**ç‰¹å¾**ï¼š

- å®¢æˆ·è¯­é€Ÿã€éŸ³é‡ï¼ˆæ€¥èºã€æ„¤æ€’ï¼‰
- å®¢æœå“åº”æ—¶é—´

### 3. äº¤æ˜“å‘˜é€šè¯åˆè§„ç›‘æ§

**ä»»åŠ¡**ï¼šæ£€æµ‹å¼‚å¸¸è¡Œä¸ºï¼ˆå¦‚å†…å¹•äº¤æ˜“æš—ç¤ºï¼‰ã€‚

**æŒ‘æˆ˜**ï¼š

- å®æ—¶æ€§è¦æ±‚é«˜
- æœ¯è¯­ä¸°å¯Œï¼ˆéœ€é¢†åŸŸ ASRï¼‰
- éšç§åˆè§„

## é£é™©ç‚¹ä¸æ³¨æ„äº‹é¡¹

### 1. ASR é”™è¯¯ä¸ä¿¡é“å™ªå£°

**ASR è¯é”™è¯¯ç‡ï¼ˆWERï¼‰**ï¼š

$$
\text{WER} = \frac{\text{æ’å…¥} + \text{åˆ é™¤} + \text{æ›¿æ¢}}{\text{æ€»è¯æ•°}}
$$

**å½±å“å› ç´ **ï¼š

- å£éŸ³ã€è¯­é€Ÿ
- èƒŒæ™¯å™ªå£°ï¼ˆç°åœºä¼šè®®ï¼‰
- ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚å…¬å¸/äº§å“åï¼‰

**ç¼“è§£**ï¼š

- ä½¿ç”¨**é¢†åŸŸå¾®è°ƒ**çš„ ASRï¼ˆå¦‚é‡‘èä¸“ç”¨æ¨¡å‹ï¼‰
- **è¯´è¯äººåˆ†ç¦»**ï¼šåŒºåˆ† CEO vs CFO vs åˆ†æå¸ˆ
- **ç½®ä¿¡åº¦è¿‡æ»¤**ï¼šåˆ é™¤ä½ç½®ä¿¡åº¦è¯†åˆ«

### 2. è¯´è¯äººåˆ†ç¦»ä¸èº«ä»½å¯¹é½

**é—®é¢˜**ï¼š

ç”µè¯ä¼šä¸­å¤šäººå‘è¨€ï¼Œéœ€åŒºåˆ†è°è¯´äº†ä»€ä¹ˆã€‚

**æŠ€æœ¯**ï¼š

- **è¯´è¯äººåˆ†æ®µï¼ˆDiarizationï¼‰**ï¼šæ ‡è®°æ¯æ®µéŸ³é¢‘çš„è¯´è¯äºº
  ```python
  from pyannote.audio import Pipeline
  
  pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")
  diarization = pipeline("audio.wav")
  
  for turn, _, speaker in diarization.itertracks(yield_label=True):
      print(f"{speaker}: {turn.start}s - {turn.end}s")
  ```

- **è¯´è¯äººè¯†åˆ«**ï¼šåŒ¹é…åˆ°å…·ä½“äººç‰©ï¼ˆCEO/CFOï¼‰

**æŒ‘æˆ˜**ï¼š

- éŸ³é¢‘è´¨é‡å·®æ—¶é”™è¯¯ç‡é«˜
- èº«ä»½æ ‡æ³¨éœ€äººå·¥éªŒè¯

### 3. å¯å¾—æ€§é€‰æ‹©åè¯¯

**é—®é¢˜**ï¼š

å¹¶éæ‰€æœ‰å…¬å¸éƒ½å…¬å¼€ç”µè¯ä¼šå½•éŸ³ã€‚

- å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€
- å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å…¬å¸å¯èƒ½ä¸å…¬å¼€

**åæœ**ï¼š

- æ ·æœ¬ä»£è¡¨æ€§å·®
- æ¨¡å‹æ¨å¹¿åˆ°å…¨å¸‚åœºæ—¶å¤±æ•ˆ

**ç¼“è§£**ï¼š

- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾ï¼ˆå¸‚å€¼ã€è¡Œä¸šåˆ†å¸ƒï¼‰
- å€¾å‘è¯„åˆ†åŠ æƒï¼ˆPropensity Score Weightingï¼‰

# è§†é¢‘æ¨¡æ€ï¼šæ—¶åºèšåˆã€è¡Œä¸ºä¿¡å·ä¸åˆè§„è¾¹ç•Œ

## è§†é¢‘ = å›¾åƒåºåˆ— + éŸ³é¢‘

**è§†é¢‘ç‰¹å¾**ï¼š

- **è§†è§‰é€šé“**ï¼šå¸§çº§å›¾åƒç‰¹å¾
- **éŸ³é¢‘é€šé“**ï¼šè¯­éŸ³/èƒŒæ™¯éŸ³ç‰¹å¾
- **æ—¶åºå»ºæ¨¡**ï¼šæ•æ‰åŠ¨æ€å˜åŒ–

## å¸§çº§è§†è§‰è¡¨å¾

### æå–å•å¸§ç‰¹å¾

```python
import cv2

# è¯»å–è§†é¢‘
cap = cv2.VideoCapture("video.mp4")

frames = []
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frames.append(frame)

cap.release()

# å¯¹æ¯å¸§æå– CNN ç‰¹å¾
from torchvision.models import resnet50
model = resnet50(pretrained=True)
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

frame_features = []
for frame in frames[::30]:  # æ¯ç§’1å¸§ï¼ˆå‡è®¾30fpsï¼‰
    tensor = preprocess(frame)
    feature = feature_extractor(tensor.unsqueeze(0)).squeeze()
    frame_features.append(feature.numpy())

# å¾—åˆ° (T, 2048) çš„ç‰¹å¾çŸ©é˜µï¼ŒT = å¸§æ•°
```

### æ—¶é—´èšåˆ

**ç®€å•æ–¹æ³•**ï¼š

- **å¹³å‡æ± åŒ–**ï¼š$\mathbf{v}_{\text{video}} = \frac{1}{T}\sum_{t=1}^T \mathbf{v}_t$
- **æœ€å¤§æ± åŒ–**ï¼š$\mathbf{v}_{\text{video}} = \max_{t=1}^T \mathbf{v}_t$

**é«˜çº§æ–¹æ³•**ï¼š

- **æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNï¼‰**
- **LSTM/GRU**ï¼šæ•æ‰é•¿æœŸä¾èµ–
- **Transformer**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶

## è¡Œä¸ºçº¿ç´¢ï¼šéè¨€è¯­ä¿¡å·

### é¢éƒ¨åŠ¨ä½œç¼–ç ï¼ˆFacial Action Units, AUsï¼‰

**å®šä¹‰**ï¼š

Paul Ekman æå‡ºçš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨ç¼–ç ç³»ç»Ÿï¼Œå…± 46 ä¸ª AUã€‚

**ç¤ºä¾‹**ï¼š

- AU1: å†…çœ‰ä¸Šæ‰¬ï¼ˆæƒŠè®¶ï¼‰
- AU4: çš±çœ‰ï¼ˆæ²‰æ€ã€æ‹…å¿§ï¼‰
- AU12: å˜´è§’ä¸Šæ‰¬ï¼ˆå¾®ç¬‘ï¼‰

**æå–å·¥å…·**ï¼š

```python
# OpenFaceï¼ˆå¼€æºï¼‰
import subprocess

subprocess.run([
    "FeatureExtraction",
    "-f", "video.mp4",
    "-out_dir", "output/"
])

# è¾“å‡º CSV æ–‡ä»¶ï¼ŒåŒ…å«æ¯å¸§çš„ AU å¼ºåº¦
import pandas as pd
aus = pd.read_csv("output/video.csv")
print(aus[['AU01_r', 'AU04_r', 'AU12_r']].head())
```

**é‡‘èåº”ç”¨**ï¼š

- CEO åœ¨è·¯æ¼”ä¸­çš„å¾®è¡¨æƒ…ï¼ˆç´§å¼ ã€è‡ªä¿¡ï¼‰
- ä¸è‚¡ä»·/èèµ„æˆåŠŸç‡çš„å…³ç³»

### å¤´éƒ¨å§¿æ€ï¼ˆHead Poseï¼‰

**ç‰¹å¾**ï¼š

- Yawï¼ˆå·¦å³è½¬ï¼‰
- Pitchï¼ˆä¸Šä¸‹ç‚¹å¤´ï¼‰
- Rollï¼ˆå·¦å³å€¾æ–œï¼‰

**å«ä¹‰**ï¼š

- é¢‘ç¹å›é¿ç›®å…‰ â†’ ä¸è¯šå®ï¼Ÿ
- é¢‘ç¹ç‚¹å¤´ â†’ è®¤åŒ/å¼ºè°ƒ

### æ³¨è§†ï¼ˆGazeï¼‰

**ç‰¹å¾**ï¼š

- çœ¼ç›æ³¨è§†æ–¹å‘
- çœ¼ç¥æ¥è§¦æ—¶é•¿

**é‡‘èåº”ç”¨**ï¼š

- ç®¡ç†å±‚å›ç­”é—®é¢˜æ—¶çš„çœ¼ç¥å˜åŒ–

## æ—¶åºå»ºæ¨¡ç¤ºä¾‹ï¼šLSTM

```python
import torch
import torch.nn as nn

class VideoClassifier(nn.Module):
    def __init__(self, input_dim=2048, hidden_dim=512, num_classes=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        _, (h_n, _) = self.lstm(x)
        # h_n: (1, batch, hidden_dim)
        out = self.fc(h_n.squeeze(0))
        return out

# è®­ç»ƒ
model = VideoClassifier()
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    for video_features, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(video_features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

## é£é™©ç‚¹ä¸åˆè§„è¾¹ç•Œ

### 1. æµ‹é‡è¯¯å·®æ¥æº

**è§’åº¦/å…‰ç…§/å‹ç¼©**ï¼š

- ä¾§è„¸æ£€æµ‹ AU ä¸å‡†
- é€†å…‰å¯¼è‡´é¢éƒ¨ä¸æ¸…æ™°
- è§†é¢‘å‹ç¼©ä¸¢å¤±ç»†èŠ‚

**é®æŒ¡**ï¼š

- æ‰‹é®ä½è„¸
- é¢å…·/å£ç½©

**ç¼“è§£**ï¼š

- è´¨é‡è¯„åˆ†ï¼šå‰”é™¤ä½è´¨é‡å¸§
- æ•°æ®å¢å¼ºï¼šè®­ç»ƒæ—¶æ¨¡æ‹Ÿå„ç§æ¡ä»¶

### 2. ç®—æ³•åå·®

**é—®é¢˜**ï¼š

é¢éƒ¨è¯†åˆ«ç®—æ³•åœ¨ä¸åŒäººç§/æ€§åˆ«ä¸Šçš„å‡†ç¡®ç‡ä¸åŒã€‚

- å¯¹ç™½äººã€ç”·æ€§çš„è¯†åˆ«ç‡æ›´é«˜
- å¯¹æ·±è‰²çš®è‚¤ã€å¥³æ€§çš„è¯†åˆ«ç‡è¾ƒä½

**åæœ**ï¼š

- æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“çš„é¢„æµ‹å­˜åœ¨ç³»ç»Ÿæ€§åå·®
- ä¼¦ç†ä¸å…¬å¹³æ€§é—®é¢˜

**ç¼“è§£**ï¼š

- ä½¿ç”¨**å…¬å¹³æ€§ä¼˜åŒ–**çš„æ¨¡å‹
- æŠ¥å‘Šä¸åŒç¾¤ä½“çš„æ€§èƒ½å·®å¼‚
- åœ¨æ•æ„Ÿåœºæ™¯ä¸­é¿å…ä½¿ç”¨

### 3. æ¶‰åŠç”Ÿç‰©ç‰¹å¾çš„åˆè§„è¾¹ç•Œ

:::{.callout-caution}
### âš ï¸ æ³•å¾‹ä¸ä¼¦ç†çº¢çº¿

**ç”Ÿç‰©ç‰¹å¾æ•°æ®**ï¼ˆé¢éƒ¨ã€å£°çº¹ã€è™¹è†œç­‰ï¼‰å—ä¸¥æ ¼ç›‘ç®¡ï¼š

**æ³•è§„**ï¼š

- **æ¬§ç›Ÿ GDPR**ï¼šéœ€æ˜ç¡®åŒæ„ï¼Œç”¨é€”å—é™
- **ä¸­å›½ã€Šä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‹**ï¼šæ•æ„Ÿä¸ªäººä¿¡æ¯ï¼Œéœ€å•ç‹¬åŒæ„
- **ç¾å›½å„å·æ³•å¾‹**ï¼šå¦‚åŠ å· CCPAã€ä¼Šåˆ©è¯ºä¼Šå· BIPA

**ç¦æ­¢åœºæ™¯**ï¼š

- æœªç»åŒæ„çš„ç§˜å¯†æ”¶é›†
- ç”¨äºæ­§è§†æ€§å†³ç­–ï¼ˆå¦‚è´·æ¬¾å®¡æ‰¹åŸºäºé¢ç›¸ï¼‰
- å¤§è§„æ¨¡å…¬å…±ç›‘æ§

**åˆè§„å»ºè®®**ï¼š

1. **æ˜ç¡®å‘ŠçŸ¥**ï¼šæ•°æ®æ”¶é›†ç›®çš„ã€ç”¨é€”
2. **è·å–åŒæ„**ï¼šå¯æ’¤å›çš„æ˜ç¡®æˆæƒ
3. **æœ€å°åŒ–åŸåˆ™**ï¼šåªæ”¶é›†å¿…è¦æ•°æ®
4. **å»æ ‡è¯†åŒ–**ï¼šå°½å¯èƒ½åŒ¿ååŒ–
5. **äººå·¥ç›‘ç£**ï¼šæ•æ„Ÿå†³ç­–éœ€äººç±»å®¡æ ¸
:::

# èåˆä¸åˆ†ç±»è¯„ä¼°ï¼šå¤šæ¨¡æ€å¢é‡ä¿¡æ¯æ£€éªŒ

## èåˆç­–ç•¥

### 1. æ—©èåˆï¼ˆEarly Fusionï¼‰

**åœ¨ç‰¹å¾å±‚é¢æ‹¼æ¥**ï¼š

```python
# å›¾åƒç‰¹å¾ (2048ç»´) + éŸ³é¢‘ç‰¹å¾ (768ç»´) + æ–‡æœ¬ç‰¹å¾ (768ç»´)
multimodal_feature = np.concatenate([
    image_embedding,
    audio_embedding,
    text_embedding
])  # (3584ç»´)

# è¾“å…¥åˆ°åˆ†ç±»å™¨
classifier.fit(multimodal_feature, label)
```

**ä¼˜ç‚¹**ï¼šç®€å•

**ç¼ºç‚¹**ï¼š

- ä¸åŒæ¨¡æ€ç»´åº¦å·®å¼‚å¤§ï¼Œå¯èƒ½ä¸»å¯¼æ€§ä¸å‡
- ç¼ºå¤±æ¨¡æ€æ—¶éš¾ä»¥å¤„ç†

### 2. ä¸­æœŸèåˆï¼ˆMid Fusionï¼‰

**å„æ¨¡æ€å…ˆå•ç‹¬ç¼–ç ï¼Œå†èåˆ**ï¼š

```python
class MidFusionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.image_encoder = nn.Linear(2048, 512)
        self.audio_encoder = nn.Linear(768, 512)
        self.text_encoder = nn.Linear(768, 512)
        self.fusion = nn.Linear(512*3, 256)
        self.classifier = nn.Linear(256, 2)
    
    def forward(self, image, audio, text):
        img_emb = self.image_encoder(image)
        aud_emb = self.audio_encoder(audio)
        txt_emb = self.text_encoder(text)
        
        fused = torch.cat([img_emb, aud_emb, txt_emb], dim=1)
        fused = F.relu(self.fusion(fused))
        return self.classifier(fused)
```

**ä¼˜ç‚¹**ï¼šå„æ¨¡æ€åœ°ä½å¹³ç­‰

### 3. åèåˆï¼ˆLate Fusionï¼‰

**å„æ¨¡æ€å•ç‹¬é¢„æµ‹ï¼Œå†é›†æˆ**ï¼š

```python
# è®­ç»ƒä¸‰ä¸ªç‹¬ç«‹åˆ†ç±»å™¨
image_pred = image_classifier.predict_proba(image_features)
audio_pred = audio_classifier.predict_proba(audio_features)
text_pred = text_classifier.predict_proba(text_features)

# åŠ æƒå¹³å‡
final_pred = 0.4 * image_pred + 0.3 * audio_pred + 0.3 * text_pred
```

**ä¼˜ç‚¹**ï¼š

- å¯è§£é‡Šæ€§å¼ºï¼ˆçŸ¥é“å„æ¨¡æ€è´¡çŒ®ï¼‰
- å®¹æ˜“å¤„ç†ç¼ºå¤±æ¨¡æ€

**ç¼ºç‚¹**ï¼š

- æœªåˆ©ç”¨æ¨¡æ€é—´äº¤äº’

### 4. æ³¨æ„åŠ›èåˆï¼ˆAttention-basedï¼‰

**è‡ªåŠ¨å­¦ä¹ å„æ¨¡æ€çš„æƒé‡**ï¼š

```python
class AttentionFusion(nn.Module):
    def __init__(self, input_dim=512):
        super().__init__()
        self.attention = nn.Linear(input_dim, 1)
    
    def forward(self, modalities):
        # modalities: list of (batch, input_dim)
        stacked = torch.stack(modalities, dim=1)  # (batch, n_modalities, input_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_scores = self.attention(stacked).squeeze(-1)  # (batch, n_modalities)
        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)  # (batch, n_modalities, 1)
        
        # åŠ æƒå’Œ
        fused = (stacked * attn_weights).sum(dim=1)  # (batch, input_dim)
        return fused
```

**ä¼˜ç‚¹**ï¼šè‡ªé€‚åº”èåˆ

## ç¼ºå¤±æ¨¡æ€å¤„ç†

**ç°å®é—®é¢˜**ï¼š

- éƒ¨åˆ†å…¬å¸æ²¡æœ‰ç”µè¯ä¼šå½•éŸ³ï¼ˆéŸ³é¢‘ç¼ºå¤±ï¼‰
- éƒ¨åˆ†æ—¶æœŸå«æ˜Ÿå›¾åƒæœ‰äº‘é®æŒ¡ï¼ˆå›¾åƒç¼ºå¤±ï¼‰

**ç­–ç•¥**ï¼š

1. **åˆ é™¤ç¼ºå¤±æ ·æœ¬**ï¼ˆç®€å•ä½†æŸå¤±æ•°æ®ï¼‰
2. **ç”¨é›¶å‘é‡å¡«å……**
3. **è®­ç»ƒå•æ¨¡æ€ + å¤šæ¨¡æ€æ¨¡å‹**ï¼Œç¼ºå¤±æ—¶ç”¨å•æ¨¡æ€

## å¢é‡è´¡çŒ®è¯„ä¼°

### å®éªŒè®¾è®¡

**åŸºå‡†æ¨¡å‹**ï¼šä»…ç”¨ç»“æ„åŒ–å˜é‡ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰

**å¢å¼ºæ¨¡å‹**ï¼š+ æ–‡æœ¬å˜é‡

**å¤šæ¨¡æ€æ¨¡å‹**ï¼š+ æ–‡æœ¬ + å›¾åƒ/éŸ³é¢‘

**æ¯”è¾ƒ**ï¼š

| æ¨¡å‹ | AUC | F1 | ç›¸å¯¹æå‡ |
|-----|-----|-----|---------|
| åŸºå‡†ï¼ˆç»“æ„åŒ–ï¼‰ | 0.75 | 0.68 | - |
| + æ–‡æœ¬ | 0.78 | 0.71 | +4% AUC |
| + æ–‡æœ¬ + éŸ³é¢‘ | 0.80 | 0.73 | +6.7% AUC |

**ç»Ÿè®¡æ£€éªŒ**ï¼š

- **DeLong æ£€éªŒ**ï¼šæ¯”è¾ƒ AUC æ˜¯å¦æ˜¾è‘—ä¸åŒ
- **é…å¯¹ t æ£€éªŒ**ï¼šæ¯”è¾ƒé¢„æµ‹è¯¯å·®

### äº’è¡¥æ€§æ£€éªŒ

**é—®é¢˜**ï¼šå¤šæ¨¡æ€ä¿¡å·æ˜¯å¦åªæ˜¯é‡å¤æ–‡æœ¬ä¿¡æ¯ï¼Ÿ

**æ£€éªŒ**ï¼š

1. **ç›¸å…³æ€§åˆ†æ**
   ```python
   corr = df[['text_sentiment', 'audio_uncertainty', 'image_activity']].corr()
   ```
   - ç›¸å…³ç³»æ•° < 0.6 ä¸ºå¥åº·

2. **åˆ†ç»„åˆ†æ**
   - åœ¨æ–‡æœ¬ä¿¡å·å¼±çš„æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘/å›¾åƒæ˜¯å¦ä½œç”¨æ›´å¼ºï¼Ÿ

3. **æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰**
   - é€æ­¥ç§»é™¤å„æ¨¡æ€ï¼Œè§‚å¯Ÿæ€§èƒ½ä¸‹é™

### è¯¯å·®åˆ†æ

**æ¡ˆä¾‹åˆ†æ**ï¼š

- å“ªäº›æ ·æœ¬é¢„æµ‹é”™è¯¯ï¼Ÿ
- æ˜¯å¦æŸç±»æ ·æœ¬ï¼ˆå¦‚å°å¸‚å€¼ï¼‰æ›´éš¾é¢„æµ‹ï¼Ÿ
- å¤šæ¨¡æ€åœ¨å“ªäº›æƒ…å†µä¸‹å¸®åŠ©æœ€å¤§ï¼Ÿ

## åˆ‡ç‰‡è¯„ä¼°ä¸ç¨³å¥æ€§

**æ—¶é—´åˆ‡ç‰‡**ï¼š

- è®­ç»ƒæœŸ vs æµ‹è¯•æœŸæ€§èƒ½å·®å¼‚
- ä¸åŒå¸‚åœºçŠ¶æ€ï¼ˆç‰›å¸‚/ç†Šå¸‚ï¼‰

**æ¨ªæˆªé¢åˆ‡ç‰‡**ï¼š

- ä¸åŒè¡Œä¸šï¼ˆç§‘æŠ€ vs åˆ¶é€ ï¼‰
- ä¸åŒè§„æ¨¡ï¼ˆå¤§å¸‚å€¼ vs å°å¸‚å€¼ï¼‰

**ç›®çš„**ï¼šç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

# æœ¬å‘¨å°ç»“

## æ ¸å¿ƒè¦ç‚¹

1. **å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡**ï¼šæ˜ç¡®æ ‡ç­¾å®šä¹‰ï¼ˆä½•æ—¶ã€å¯¹è°ã€é¢„æµ‹ä»€ä¹ˆï¼‰
2. **å›¾åƒç‰¹å¾**ï¼šCNN/ViT/CLIPï¼Œæ³¨æ„ç©ºé—´å¯¹é½ã€è´¨é‡æ§åˆ¶ã€åŸŸæ¼‚ç§»
3. **éŸ³é¢‘ç‰¹å¾**ï¼šASR â†’ æ–‡æœ¬ vs å£°å­¦/éŸµå¾‹ç‰¹å¾ï¼Œæ³¨æ„ ASR é”™è¯¯ã€è¯´è¯äººåˆ†ç¦»
4. **è§†é¢‘ç‰¹å¾**ï¼šå¸§çº§ + æ—¶åºå»ºæ¨¡ï¼Œè¡Œä¸ºçº¿ç´¢ï¼ˆAU/å§¿æ€/æ³¨è§†ï¼‰ï¼Œæ³¨æ„æµ‹é‡è¯¯å·®
5. **èåˆç­–ç•¥**ï¼šæ—©/ä¸­/åèåˆã€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤„ç†ç¼ºå¤±æ¨¡æ€
6. **å¢é‡æ£€éªŒ**ï¼šç›¸å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡çš„å¢é‡è´¡çŒ®ï¼Œäº’è¡¥æ€§è¯æ®
7. **ä¼¦ç†è¾¹ç•Œ**ï¼šç”Ÿç‰©ç‰¹å¾åˆè§„ã€ç®—æ³•åå·®ã€éšç§ä¿æŠ¤

## æœ¬å‘¨æ€è€ƒé¢˜

### é—®é¢˜ 1

ä»¥"ä¸šç»©ç”µè¯ä¼šéŸ³é¢‘"æ„é€ ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚è¯†åˆ«ç®¡ç†å±‚ä¸ç¡®å®šæ€§çŠ¶æ€ï¼‰æ—¶ï¼Œæ ‡ç­¾é€šå¸¸å¦‚ä½•å®šä¹‰ï¼Ÿå“ªäº›ç¯èŠ‚æœ€å®¹æ˜“å¼•å…¥æ ·æœ¬é€‰æ‹©åè¯¯ï¼Ÿ

:::{.callout-note collapse="true"}
### ğŸ’¡ å‚è€ƒç­”æ¡ˆ

**æ ‡ç­¾å®šä¹‰**ï¼š

1. **åŸºäºæœªæ¥è‚¡ä»·**ï¼š
   - $y = \mathbb{I}(r_{t+1 \to t+k} < \text{å¸‚åœºä¸­ä½æ•°})$
   - ä¸ç¡®å®šæ€§é«˜ â†’ æœªæ¥è¡¨ç°å·®

2. **åŸºäºè´¢åŠ¡æ„å¤–**ï¼š
   - $y = \mathbb{I}(|\text{å®é™…EPS} - \text{é¢„æœŸEPS}| > \text{é˜ˆå€¼})$
   - ä¸ç¡®å®šæ€§é«˜ â†’ é¢„æµ‹è¯¯å·®å¤§

3. **åŸºäºæ–‡æœ¬è¯å…¸**ï¼ˆè¾…åŠ©æ ‡æ³¨ï¼‰ï¼š
   - ç»Ÿè®¡ä¸ç¡®å®šæ€§è¯æ±‡ï¼ˆ"uncertain", "might", "could"ï¼‰
   - äººå·¥éªŒè¯éƒ¨åˆ†æ ·æœ¬

**æ ·æœ¬é€‰æ‹©åè¯¯æ¥æº**ï¼š

1. **å¯å¾—æ€§åè¯¯**
   - å¤§å…¬å¸ã€é€æ˜åº¦é«˜çš„å…¬å¸æ›´å¯èƒ½å…¬å¼€å½•éŸ³
   - å°å…¬å¸ã€æœ‰è´Ÿé¢æ¶ˆæ¯çš„å¯èƒ½éšè—

2. **å­˜æ´»åè¯¯**
   - é€€å¸‚å…¬å¸çš„ç”µè¯ä¼šæ•°æ®ç¼ºå¤±

3. **æ—¶æœŸé€‰æ‹©**
   - å±æœºæœŸé—´ç”µè¯ä¼šå–æ¶ˆç‡æ›´é«˜

**ç¼“è§£**ï¼š

- æŠ¥å‘Šæ ·æœ¬ç‰¹å¾åˆ†å¸ƒ
- å€¾å‘è¯„åˆ†åŠ æƒ
- ç¨³å¥æ€§æ£€éªŒï¼ˆä»…ç”¨æŒç»­æœ‰æ•°æ®çš„å…¬å¸ï¼‰
:::

### é—®é¢˜ 2

å¦‚ä½•ä¸¥è°¨åœ°è¯æ˜å¤šæ¨¡æ€ä¿¡å·å¯¹æ–‡æœ¬/ç»“æ„åŒ–å˜é‡å…·æœ‰å¢é‡ä¿¡æ¯ï¼Ÿè¯·ç»™å‡ºä¸€ä¸ªä½ è®¤ä¸ºå¿…è¦çš„äº’è¡¥æ€§æ£€éªŒè®¾è®¡ã€‚

:::{.callout-note collapse="true"}
### ğŸ’¡ å‚è€ƒç­”æ¡ˆ

**å®Œæ•´æ£€éªŒæµç¨‹**ï¼š

**1. åŸºå‡†å¯¹æ¯”**

| æ¨¡å‹ | ç‰¹å¾ | AUC | $\Delta$ AUC |
|-----|------|-----|-------------|
| M1 | ç»“æ„åŒ–ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰ | 0.72 | - |
| M2 | M1 + æ–‡æœ¬ | 0.76 | +0.04 |
| M3 | M2 + éŸ³é¢‘ | 0.79 | +0.03 |

- **æ¡ä»¶**ï¼š$\Delta$ AUC ç»Ÿè®¡æ˜¾è‘—ï¼ˆDeLong æ£€éªŒ $p < 0.05$ï¼‰

**2. ç›¸å…³æ€§åˆ†æ**

```python
corr_matrix = df[['structured', 'text', 'audio']].corr()
assert corr_matrix.loc['audio', 'text'] < 0.6  # é¿å…é«˜åº¦å†—ä½™
```

**3. æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ**

åœ¨æ§åˆ¶æ–‡æœ¬åï¼ŒéŸ³é¢‘æ˜¯å¦ä»æœ‰é¢„æµ‹åŠ›ï¼Ÿ

$$
\text{Logit}(y) = \beta_0 + \beta_1 \text{Text} + \beta_2 \text{Audio} + \varepsilon
$$

- æ£€éªŒï¼š$\beta_2$ æ˜¯å¦æ˜¾è‘—ï¼Ÿ

**4. åˆ†ç»„å¼‚è´¨æ€§**

åœ¨æ–‡æœ¬ä¿¡å·å¼±çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘è´¡çŒ®æ›´å¤§ï¼š

```
åˆ†ç»„ï¼šæ–‡æœ¬æƒ…æ„Ÿ = ä¸­æ€§ï¼ˆä¿¡æ¯é‡å°‘ï¼‰
  â†’ éŸ³é¢‘ AUC æå‡æ›´æ˜¾è‘—
```

**5. æ¶ˆèå®éªŒ**

| ç§»é™¤æ¨¡æ€ | AUC ä¸‹é™ | è§£é‡Š |
|---------|---------|-----|
| å»é™¤éŸ³é¢‘ | -0.03 | éŸ³é¢‘æœ‰ç‹¬ç‰¹è´¡çŒ® |
| å»é™¤æ–‡æœ¬ | -0.04 | æ–‡æœ¬ä»é‡è¦ |
| å»é™¤ç»“æ„åŒ– | -0.05 | åŸºç¡€å˜é‡æœ€é‡è¦ |

**ç»“è®ºæ¨¡æ¿**ï¼š

"éŸ³é¢‘ç‰¹å¾åœ¨æ§åˆ¶æ–‡æœ¬ä¸ç»“æ„åŒ–å˜é‡åï¼Œä»èƒ½å¸¦æ¥ç»Ÿè®¡æ˜¾è‘—çš„ AUC æå‡ï¼ˆ+0.03, p=0.01ï¼‰ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºéŸ³é¢‘ä¸æ–‡æœ¬ç›¸å…³ç³»æ•°ä¸º 0.45ï¼Œè¡¨æ˜äºŒè€…æ•æ‰äº†éƒ¨åˆ†é‡å ä½†éå®Œå…¨å†—ä½™çš„ä¿¡æ¯ã€‚åœ¨æ–‡æœ¬æƒ…æ„Ÿä¸­æ€§çš„å­æ ·æœ¬ä¸­ï¼ŒéŸ³é¢‘çš„å¢é‡è´¡çŒ®æ›´å¤§ï¼ˆAUC +0.05ï¼‰ï¼Œè¯å®äº†äº’è¡¥æ€§ã€‚"
:::

---

## è¯¾ç¨‹å‰åŠæ®µå›é¡¾

å‰å››å‘¨æˆ‘ä»¬å»ºç«‹äº†**AI èµ‹èƒ½é‡‘èç ”ç©¶çš„å®Œæ•´æ¡†æ¶**ï¼š

- **ç¬¬1å‘¨**ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆi.i.d. åœºæ™¯ï¼‰
- **ç¬¬2å‘¨**ï¼šé‡‘èæ—¶åºè¯„ä¼°ä¸å›æµ‹è§„èŒƒ
- **ç¬¬3å‘¨**ï¼šæ–‡æœ¬åˆ†æçš„å¯å®¡è®¡æµç¨‹
- **ç¬¬4å‘¨**ï¼šå¤šæ¨¡æ€ä¿¡å·çš„å¢é‡æ£€éªŒ

**å…±åŒä¸»çº¿**ï¼š

1. **æ³›åŒ–èƒ½åŠ›**ï¼šæ ·æœ¬å¤–è¡¨ç°æ˜¯å”¯ä¸€æ ‡å‡†
2. **å¯å®¡è®¡æ€§**ï¼šä»åŸå§‹æ•°æ®åˆ°å˜é‡çš„æ¯ä¸€æ­¥å¯è¿½æº¯
3. **å¢é‡ä»·å€¼**ï¼šæ–°æ–¹æ³•å¿…é¡»ç›¸å¯¹å·²çŸ¥æ–¹æ³•å±•ç¤ºå¢é‡ä¿¡æ¯

## ååŠæ®µè¯¾ç¨‹é¢„å‘Š

**ç¬¬5-8å‘¨**ï¼šå­¦ç”Ÿç ”ç©¶è®¡åˆ’æ±‡æŠ¥ä¸ç ”è®¨

- æ¯ç»„å±•ç¤ºç ”ç©¶é—®é¢˜ã€æ•°æ®ã€æ–¹æ³•ã€è¯„ä¼°åè®®
- è¯¾å ‚è®¨è®ºä¸åé¦ˆ
- å¼ºåŒ–**æœ€å°è¯æ®é“¾**æ„è¯†

**ç¬¬5å‘¨å‰**å°†æœ‰**è¯¾å‰æµ‹éªŒ**ï¼Œè¦†ç›–ç¬¬1-4å‘¨æ ¸å¿ƒæ¦‚å¿µï¼Œè¯·æå‰å¤ä¹ ï¼

---

**ç¥å­¦ä¹ é¡ºåˆ©ï¼æœŸå¾…çœ‹åˆ°ä½ ä»¬çš„ç²¾å½©ç ”ç©¶ï¼**
